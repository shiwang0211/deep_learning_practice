{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# **Practice with eager execution **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n",
      "Eager execution: True\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Compute with concrete values\n",
    "- Doesn't break the flow of computing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.]] <class 'EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "x = [[2.]]\n",
    "m = tf.matmul(x,x)\n",
    "print(\"{}\".format(m), type(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=528474, shape=(2, 2), dtype=int32, numpy=\n",
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([[1,2],[3,4]])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]], dtype=int32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = x.numpy()\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  4],\n",
       "       [ 9, 16]], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(x,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dataset API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Build complex input pipelines\n",
    "- Two new abstractions\n",
    "        - tf.data.Dataset\n",
    "        - tf.data.Iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local copy of the dataset file: /Users/shiwang/.keras/datasets/iris_training.csv\n",
      "Local copy of the dataset file: /Users/shiwang/.keras/datasets/iris_test.csv\n"
     ]
    }
   ],
   "source": [
    "train_dataset_url = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "test_dataset_url = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "train_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(train_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "test_dataset_fp = tf.keras.utils.get_file(fname=os.path.basename(test_dataset_url),\n",
    "                                           origin=train_dataset_url)\n",
    "print(\"Local copy of the dataset file: {}\".format(train_dataset_fp))\n",
    "print(\"Local copy of the dataset file: {}\".format(test_dataset_fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120,4,setosa,versicolor,virginica\n",
      "6.4,2.8,5.6,2.2,2\n",
      "5.0,2.3,3.3,1.0,1\n",
      "4.9,2.5,4.5,1.7,2\n",
      "4.9,3.1,1.5,0.1,0\n"
     ]
    }
   ],
   "source": [
    "!head -n5 {train_dataset_fp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_csv(line):\n",
    "    example_defaults = [[0.], [0.], [0.], [0.], [0]]  # sets field types\n",
    "    parsed_line = tf.decode_csv(line, example_defaults)\n",
    "    \n",
    "    # First 4 fields are features, combine into single tensor\n",
    "    features = tf.reshape(parsed_line[:-1], shape=(4,))\n",
    "    \n",
    "    # Last field is the label\n",
    "    label = tf.reshape(parsed_line[-1], shape=())\n",
    "    return features, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.TextLineDataset(train_dataset_fp).skip(1).map(parse_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1056979, shape=(4,), dtype=float32, numpy=array([ 6.4000001 ,  2.79999995,  5.5999999 ,  2.20000005], dtype=float32)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features, label = iter(train_dataset).next()\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1039640, shape=(), dtype=int32, numpy=2>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = train_dataset.shuffle(buffer_size=1000).repeat(2).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example features: tf.Tensor([ 5.19999981  3.4000001   1.39999998  0.2       ], shape=(4,), dtype=float32)\n",
      "example label: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "features, label = iter(train_dataset).next()\n",
    "print(\"example features:\", features[0])\n",
    "print(\"example label:\", label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- High level neural network APU\n",
    "- Run on top of Tensorflow, CNTK, Theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "layer1 = tf.keras.layers.Dense(10, activation=\"relu\", input_shape=(4,)) # input shape required\n",
    "layer2 = tf.keras.layers.Dense(10, activation=\"relu\")\n",
    "layer3 = tf.keras.layers.Dense(3)\n",
    "model.add(layer1)\n",
    "model.add(layer2)\n",
    "model.add(layer3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Cannot use the following with Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(loss='categorical_crossentropy',\n",
    "#              optimizer='adam',\n",
    "#              metrics=['accuracy'])\n",
    "#\n",
    "#model.fit(features, labels, nb_epoch=10, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Only native loss and optimizer is compatible with eager execution\n",
    "- The grad function uses the loss function and the tf.GradientTape to record operations that compute the gradients used to optimize our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "    y_ = model(x)\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels = y, logits = y_)\n",
    "\n",
    "def grad(model, inputs, targets):\n",
    "    with tf.GradientTape() as tape: # all forward-pass operations get recorded to a \"tape\n",
    "        loss_value = loss(model, inputs, targets)\n",
    "    return tape.gradient(loss_value, model.variables)\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 000: Loss: 1.028, Accuracy: 59.583%\n",
      "Epoch 050: Loss: 0.226, Accuracy: 95.833%\n",
      "Epoch 100: Loss: 0.117, Accuracy: 98.333%\n",
      "Epoch 150: Loss: 0.095, Accuracy: 97.917%\n",
      "Epoch 200: Loss: 0.070, Accuracy: 98.333%\n",
      "Epoch 250: Loss: 0.077, Accuracy: 98.750%\n",
      "Epoch 300: Loss: 0.067, Accuracy: 98.333%\n",
      "Epoch 350: Loss: 0.062, Accuracy: 97.917%\n"
     ]
    }
   ],
   "source": [
    "# keep results for plotting\n",
    "train_loss_results = []\n",
    "train_accuracy_results = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # For each epoch, run over Entire dataset\n",
    "    epoch_loss_avg = tfe.metrics.Mean()\n",
    "    epoch_accuracy = tfe.metrics.Accuracy()\n",
    "    \n",
    "    # For batch size of 32\n",
    "    for x, y in train_dataset:  # return a batch at every call of train_dataset\n",
    "        \n",
    "        # Call our gradient function, and Optimize for model.variables\n",
    "        grads = grad(model, x, y)\n",
    "        optimizer.apply_gradients(zip(grads, model.variables), \n",
    "                                 global_step = tf.train.get_or_create_global_step())\n",
    "        \n",
    "        # Track progress\n",
    "        ### Print out value at Run Time\n",
    "        epoch_loss_avg(loss(model, x, y))\n",
    "        epoch_accuracy(tf.argmax(model(x), axis = 1, output_type=tf.int32), \n",
    "                       y)\n",
    "        \n",
    "    # end epoch\n",
    "    train_loss_results.append(epoch_loss_avg.result())\n",
    "    train_accuracy_results.append(epoch_accuracy.result())\n",
    "\n",
    "    if epoch % 50 == 0:\n",
    "        print(\"Epoch {:03d}: Loss: {:.3f}, Accuracy: {:.3%}\".format(epoch,\n",
    "                                                                epoch_loss_avg.result(),\n",
    "                                                                epoch_accuracy.result()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at some of the Eager Execution outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1039487, shape=(), dtype=float32, numpy=0.033696391>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(model, x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor: id=1039575, shape=(4, 10), dtype=float32, numpy=\n",
       " array([[ 0.        , -0.02939828,  0.        ,  0.        ,  0.        ,\n",
       "         -0.01705971,  0.        , -0.06066043,  0.06475139,  0.        ],\n",
       "        [ 0.        , -0.00836889,  0.        ,  0.        ,  0.        ,\n",
       "         -0.00403176,  0.        , -0.01694808,  0.01688088,  0.        ],\n",
       "        [ 0.        , -0.02166644,  0.        ,  0.        ,  0.        ,\n",
       "         -0.01374358,  0.        , -0.04480889,  0.04893772,  0.        ],\n",
       "        [ 0.        , -0.01786771,  0.        ,  0.        ,  0.        ,\n",
       "         -0.01130618,  0.        , -0.03771346,  0.04218885,  0.        ]], dtype=float32)>,\n",
       " <tf.Tensor: id=1039573, shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.        , -0.00164259,  0.        ,  0.        ,  0.        ,\n",
       "        -0.00072838,  0.        , -0.00316354,  0.00282967,  0.        ], dtype=float32)>,\n",
       " <tf.Tensor: id=1039571, shape=(10, 10), dtype=float32, numpy=\n",
       " array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.00936601,  0.01021489, -0.00442977,  0.00026778,\n",
       "          0.00414223,  0.        ,  0.        , -0.01929157, -0.00461735],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.02034845,  0.02179982, -0.00938987, -0.00042072,\n",
       "          0.00895287, -0.00077113,  0.        , -0.04213975, -0.01009741],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ],\n",
       "        [ 0.        ,  0.01982666,  0.02147966, -0.00922644,  0.00047392,\n",
       "          0.00875154,  0.        ,  0.        , -0.04061987, -0.00953969],\n",
       "        [ 0.        ,  0.00960336,  0.00941441, -0.00401545, -0.00279535,\n",
       "          0.0041219 , -0.00244147,  0.        , -0.02067026, -0.00527118],\n",
       "        [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "          0.        ,  0.        ,  0.        ,  0.        ,  0.        ]], dtype=float32)>,\n",
       " <tf.Tensor: id=1039569, shape=(10,), dtype=float32, numpy=\n",
       " array([ 0.        ,  0.00072959,  0.00073945, -0.00035948, -0.00033858,\n",
       "         0.00031602, -0.0003184 ,  0.        , -0.00173666, -0.0005627 ], dtype=float32)>,\n",
       " <tf.Tensor: id=1039567, shape=(10, 3), dtype=float32, numpy=\n",
       " array([[  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [ -9.94027127e-04,   6.89677894e-03,  -5.90279326e-03],\n",
       "        [ -1.34891924e-03,  -6.23531453e-03,   7.58421980e-03],\n",
       "        [  1.37150846e-03,   2.16774233e-02,  -2.30489522e-02],\n",
       "        [ -1.54105120e-03,   1.53584324e-03,   5.20150979e-06],\n",
       "        [ -5.22329821e-04,   4.85709123e-03,  -4.33477759e-03],\n",
       "        [ -4.54069173e-04,   4.54051245e-04,   1.50970987e-08],\n",
       "        [  0.00000000e+00,   0.00000000e+00,   0.00000000e+00],\n",
       "        [  1.71101245e-04,   2.47317590e-02,  -2.49028709e-02],\n",
       "        [  1.57299358e-03,   2.58005541e-02,  -2.73735840e-02]], dtype=float32)>,\n",
       " <tf.Tensor: id=1039565, shape=(3,), dtype=float32, numpy=array([ -3.32308118e-05,   7.89811835e-04,  -7.56586378e-04], dtype=float32)>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grad(model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict with model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0 prediction: Iris setosa\n",
      "Example 1 prediction: Iris versicolor\n",
      "Example 2 prediction: Iris virginica\n"
     ]
    }
   ],
   "source": [
    "class_ids = [\"Iris setosa\", \"Iris versicolor\", \"Iris virginica\"]\n",
    "\n",
    "predict_dataset = tf.convert_to_tensor([\n",
    "    [5.1, 3.3, 1.7, 0.5,],\n",
    "    [5.9, 3.0, 4.2, 1.5,],\n",
    "    [6.9, 3.1, 5.4, 2.1]\n",
    "])\n",
    "\n",
    "predictions = model(predict_dataset)\n",
    "\n",
    "for i, logits in enumerate(predictions):\n",
    "    class_idx = tf.argmax(logits).numpy()\n",
    "    name = class_ids[class_idx]\n",
    "    print(\"Example {} prediction: {}\".format(i, name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisit Eager Execution with a simple example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revist Gradient Tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=1039677, shape=(1, 1), dtype=float32, numpy=array([[ 2.]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "w = tfe.Variable([[1.0]])\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = w * w\n",
    "\n",
    "grad = tape.gradient(loss, [w])\n",
    "print(grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Customized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.W = tfe.Variable(5., name = 'W')\n",
    "        self.b = tfe.Variable(10., name = 'b')\n",
    "    \n",
    "    def predict(self, inputs):\n",
    "        return inputs * self.W + self.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sample inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=1039693, shape=(), dtype=float32, numpy=4.7433739>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_inputs = tf.random_normal([1000])\n",
    "noise = tf.random_normal([1000])\n",
    "training_outputs = training_inputs * 3 + 2 + noise\n",
    "training_outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, y):\n",
    "    error = model.predict(x) - y\n",
    "    return tf.reduce_mean(tf.square(error))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(model, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss_ = loss(model, x, y)\n",
    "    return tape.gradient(loss_, [model.W, model.b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Optimizer and Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: 71.751\n"
     ]
    }
   ],
   "source": [
    "model = Model()\n",
    "opt = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
    "print(\"Initial loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss at step 000: 68.835\n",
      "Loss at step 020: 30.275\n",
      "Loss at step 040: 13.659\n",
      "Loss at step 060: 6.478\n",
      "Loss at step 080: 3.364\n",
      "Loss at step 100: 2.010\n",
      "Loss at step 120: 1.419\n",
      "Loss at step 140: 1.161\n",
      "Loss at step 160: 1.047\n",
      "Loss at step 180: 0.997\n",
      "Loss at step 200: 0.974\n",
      "Loss at step 220: 0.965\n",
      "Loss at step 240: 0.960\n",
      "Loss at step 260: 0.958\n",
      "Loss at step 280: 0.957\n",
      "Loss at step 300: 0.957\n",
      "Loss at step 320: 0.957\n",
      "Loss at step 340: 0.957\n",
      "Loss at step 360: 0.957\n",
      "Loss at step 380: 0.957\n",
      "Loss at step 400: 0.957\n",
      "Loss at step 420: 0.957\n",
      "Loss at step 440: 0.957\n",
      "Loss at step 460: 0.957\n",
      "Loss at step 480: 0.957\n",
      "Final loss: 0.957\n",
      "W = 2.9841554164886475, B = 1.983044147491455\n"
     ]
    }
   ],
   "source": [
    "for i in range(500):\n",
    "    grads = grad(model, training_inputs, training_outputs)\n",
    "    opt.apply_gradients(zip(grads, [model.W, model.b]), \n",
    "                        global_step=tf.train.get_or_create_global_step())\n",
    "    \n",
    "    if i % 20 == 0:\n",
    "        print(\"Loss at step {:03d}: {:.3f}\".format(i, loss(model, training_inputs, training_outputs)))\n",
    "\n",
    "print(\"Final loss: {:.3f}\".format(loss(model, training_inputs, training_outputs)))\n",
    "print(\"W = {}, B = {}\".format(model.W.numpy(), model.b.numpy()))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practice with Estimator API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/tensorflow_programming_environment.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 1.8.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "print(\"TensorFlow version: {}\".format(tf.VERSION))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data as Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_URL = \"http://download.tensorflow.org/data/iris_training.csv\"\n",
    "TEST_URL = \"http://download.tensorflow.org/data/iris_test.csv\"\n",
    "\n",
    "CSV_COLUMN_NAMES = ['SepalLength', 'SepalWidth',\n",
    "                    'PetalLength', 'PetalWidth', 'Species']\n",
    "\n",
    "def load_data(label_name='Species'):\n",
    "    # Just copies a remote CSV file to a local file system\n",
    "    train_path = tf.keras.utils.get_file(fname=TRAIN_URL.split('/')[-1],\n",
    "                                         origin=TRAIN_URL)\n",
    "    train = pd.read_csv(filepath_or_buffer=train_path,\n",
    "                        names=CSV_COLUMN_NAMES,  # list of column names\n",
    "                        header=0  # ignore the first row of the CSV file.\n",
    "                       )\n",
    "    train_features, train_label = train, train.pop(label_name)\n",
    "    test_path = tf.keras.utils.get_file(TEST_URL.split('/')[-1], TEST_URL)\n",
    "    test = pd.read_csv(test_path, names=CSV_COLUMN_NAMES, header=0)\n",
    "    test_features, test_label = test, test.pop(label_name)\n",
    "    return (train_features, train_label), (test_features, test_label)\n",
    "\n",
    "(train_feature, train_label), (test_feature, test_label) = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape # Just pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLength</th>\n",
       "      <th>SepalWidth</th>\n",
       "      <th>PetalLength</th>\n",
       "      <th>PetalWidth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLength  SepalWidth  PetalLength  PetalWidth\n",
       "0          5.9         3.0          4.2         1.5\n",
       "1          6.9         3.1          5.4         2.1\n",
       "2          5.1         3.3          1.7         0.5"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_feature.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `Feature Column`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/feature_columns/inputs_to_model_bridge.jpg\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret the data in each feature as its literal floating-point value\n",
    "my_feature_columns = [\n",
    "    tf.feature_column.numeric_column(key='SepalLength',dtype=tf.float64),\n",
    "    tf.feature_column.numeric_column(key='SepalWidth'),\n",
    "    tf.feature_column.numeric_column(key='PetalLength'),\n",
    "    tf.feature_column.numeric_column(key='PetalWidth')\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Other examples of feature_column for categorical columns*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.tensorflow.org/images/feature_columns/categorical_column_with_identity.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.tensorflow.org/get_started/feature_columns\n",
    "# Feature B is categorical\n",
    "identity_feature_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='my_feature_b',\n",
    "    num_buckets=4) # Values [0, 4)\n",
    "\n",
    "def input_fn():\n",
    "    ...\n",
    "    return ({ 'my_feature_a':[7, 9, 5, 2], 'my_feature_b':[3, 1, 2, 2] },\n",
    "            [Label_values])\n",
    "\n",
    "# Feature B is a word\n",
    "vocabulary_feature_column = tf.feature_column.categorical_column_with_vocabulary_file(\n",
    "        key='my_feature_b',\n",
    "        #vocabulary_list=[\"kitchenware\", \"electronics\", \"sports\"]\n",
    "        vocabulary_file=\"product_class.txt\",\n",
    "        vocabulary_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `Classifier`: Just use DNN Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmp53pto_yh\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmp53pto_yh', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x18144d6550>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "# DNNClassifer is a pre-made Estimator \n",
    "# https://www.tensorflow.org/api_docs/python/tf/estimator/DNNClassifier\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    feature_columns=my_feature_columns,\n",
    "    hidden_units=[10, 10], # 2 hidden layers\n",
    "    optimizer='Adagrad',\n",
    "    n_classes=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `Input` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn(features, labels, batch_size): # Inputs are pandas df and series\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels)) # Using Dataset API\n",
    "    dataset = dataset.shuffle(buffer_size=1000).repeat(count=None).batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next() # return a batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train_feature is a Python dictionary in which:\n",
    "    - Each key is the name of a feature.\n",
    "    - Each value is an array containing the values for each example in the training set.\n",
    "\n",
    "- Train_label is an array containing the values of the label for every example in the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert into dictionary\n",
    "dict(train_feature)['PetalLength'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'PetalLength': <tf.Tensor 'IteratorGetNext_1:0' shape=(?,) dtype=float64>,\n",
       "  'PetalWidth': <tf.Tensor 'IteratorGetNext_1:1' shape=(?,) dtype=float64>,\n",
       "  'SepalLength': <tf.Tensor 'IteratorGetNext_1:2' shape=(?,) dtype=float64>,\n",
       "  'SepalWidth': <tf.Tensor 'IteratorGetNext_1:3' shape=(?,) dtype=float64>},\n",
       " <tf.Tensor 'IteratorGetNext_1:4' shape=(?,) dtype=int64>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output format\n",
    "train_input_fn(train_feature, train_label, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example to replace the batch above:\n",
    "def input_evaluation_set():\n",
    "    features = {'SepalLength': np.array([6.4, 5.0]),\n",
    "                'SepalWidth':  np.array([2.8, 2.3]),\n",
    "                'PetalLength': np.array([5.6, 3.3]),\n",
    "                'PetalWidth':  np.array([2.2, 1.0])}\n",
    "    labels = np.array([2, 1])\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_input_fn(features, labels=None, batch_size=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset.make_one_shot_iterator().get_next()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A lambda to capture the arguments while providing an input function that takes no arguments, as expected by the Estimator. \n",
    "- The steps argument tells the method to stop training after a number of training steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpo55yo6y_/model.ckpt-3000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 3001 into /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpo55yo6y_/model.ckpt.\n",
      "INFO:tensorflow:loss = 0.00118172, step = 3001\n",
      "INFO:tensorflow:global_step/sec: 933.322\n",
      "INFO:tensorflow:loss = 0.00113752, step = 3101 (0.108 sec)\n",
      "INFO:tensorflow:global_step/sec: 2325.74\n",
      "INFO:tensorflow:loss = 0.0010969, step = 3201 (0.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 2116.19\n",
      "INFO:tensorflow:loss = 0.00105913, step = 3301 (0.047 sec)\n",
      "INFO:tensorflow:global_step/sec: 2331.54\n",
      "INFO:tensorflow:loss = 0.00102434, step = 3401 (0.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 2365.12\n",
      "INFO:tensorflow:loss = 0.000991813, step = 3501 (0.042 sec)\n",
      "INFO:tensorflow:global_step/sec: 2526.03\n",
      "INFO:tensorflow:loss = 0.000961549, step = 3601 (0.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 2346.58\n",
      "INFO:tensorflow:loss = 0.000933071, step = 3701 (0.043 sec)\n",
      "INFO:tensorflow:global_step/sec: 2481.65\n",
      "INFO:tensorflow:loss = 0.000906381, step = 3801 (0.040 sec)\n",
      "INFO:tensorflow:global_step/sec: 1914.06\n",
      "INFO:tensorflow:loss = 0.000881239, step = 3901 (0.053 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 4000 into /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpo55yo6y_/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.000863604.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.canned.dnn.DNNClassifier at 0x1814502cf8>"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 30\n",
    "train_steps = 1000\n",
    "\n",
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(train_feature, train_label, batch_size),\n",
    "    steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate Model with test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-26-22:34:10\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpo55yo6y_/model.ckpt-2000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-26-22:34:10\n",
      "INFO:tensorflow:Saving dict for global step 2000: accuracy = 0.966667, average_loss = 0.0541685, global_step = 2000, loss = 1.62506\n",
      "\n",
      "Test set accuracy: 0.967\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_result = classifier.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(test_feature, test_label, batch_size))\n",
    "\n",
    "print('\\nTest set accuracy: {accuracy:0.3f}\\n'.format(**eval_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next Steps: Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisit Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((dict(train_feature), train_label)) # Using Dataset API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<RepeatDataset shapes: ({SepalLength: (), SepalWidth: (), PetalLength: (), PetalWidth: ()}, ()), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.shuffle(buffer_size=1000).repeat(count=None)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BatchDataset shapes: ({SepalLength: (?,), SepalWidth: (?,), PetalLength: (?,), PetalWidth: (?,)}, (?,)), types: ({SepalLength: tf.float64, SepalWidth: tf.float64, PetalLength: tf.float64, PetalWidth: tf.float64}, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "#Note that the dataset has an unknown batch size because the last batch will have fewer elements.\n",
    "print(dataset.batch(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# At this point the Dataset contains (features_dict, labels) pairs.\n",
    "# return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customized Estimators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_model_fn(\n",
    "   features, # This is batch_features from input_fn\n",
    "   labels,   # This is batch_labels from input_fn\n",
    "   mode,     # An instance of tf.estimator.ModeKeys\n",
    "   params):  # Additional configuration\n",
    "    \n",
    "    ### 1. Define Input, Hidden Layer, Output\n",
    "    \n",
    "    # Use `input_layer` to apply the feature columns.\n",
    "    net = tf.feature_column.input_layer(features, params['feature_columns']) # Numeric, etc\n",
    "    \n",
    "    # Build the hidden layers, sized according to the 'hidden_units' param.\n",
    "    for units in params['hidden_units']: # [10,10]\n",
    "        net = tf.layers.dense(net, units=units, activation=tf.nn.relu)\n",
    "        \n",
    "    # Compute logits (1 per class).\n",
    "    logits = tf.layers.dense(net, params['n_classes'], activation=None) # n_class = 3\n",
    "    \n",
    "    # Compute Other metrics: Loss, Pred, Acc\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predicted_classes = tf.argmax(logits, 1)\n",
    "    accuracy = tf.metrics.accuracy(labels=labels,predictions=predicted_classes,name='acc_op')\n",
    "    \n",
    "    ### 2. Define Prediction\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {\n",
    "            'class_ids': predicted_classes[:, tf.newaxis], #[0,1,2,3]\n",
    "            'probabilities': tf.nn.softmax(logits), ## Softmax\n",
    "            'logits': logits #[-,+]\n",
    "        }\n",
    "        return tf.estimator.EstimatorSpec(mode, predictions=predictions) #Notice Return Type\n",
    "\n",
    "\n",
    "    ### 3. Define Evaluation\n",
    "    metrics = {'accuracy': accuracy}\n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(\n",
    "            mode, loss=loss, eval_metric_ops=metrics)\n",
    "\n",
    "\n",
    "    ### 4. Define Train\n",
    "    optimizer = tf.train.AdagradOptimizer(learning_rate=0.1)\n",
    "    train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "    return tf.estimator.EstimatorSpec(mode, loss=loss, train_op=train_op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using default config.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmp0brj60st\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmp0brj60st', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x1814ba0d30>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n"
     ]
    }
   ],
   "source": [
    "classifier = tf.estimator.Estimator(\n",
    "    model_fn=my_model_fn,\n",
    "    params={\n",
    "        'feature_columns': my_feature_columns,\n",
    "        'hidden_units': [10, 10],\n",
    "        'n_classes': 3,\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already Defined\n",
    "#train_feature, train_label\n",
    "#def train_input_fn:\n",
    "#def eval_input_fn:\n",
    "#my_feature_columns = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpxkfol9hw/model.ckpt.\n",
      "INFO:tensorflow:loss = 3.24971, step = 1\n",
      "INFO:tensorflow:global_step/sec: 868.228\n",
      "INFO:tensorflow:loss = 0.185193, step = 101 (0.117 sec)\n",
      "INFO:tensorflow:global_step/sec: 1557.07\n",
      "INFO:tensorflow:loss = 0.133819, step = 201 (0.064 sec)\n",
      "INFO:tensorflow:global_step/sec: 1190.8\n",
      "INFO:tensorflow:loss = 0.0371454, step = 301 (0.084 sec)\n",
      "INFO:tensorflow:global_step/sec: 1208.18\n",
      "INFO:tensorflow:loss = 0.0974107, step = 401 (0.083 sec)\n",
      "INFO:tensorflow:global_step/sec: 1142.23\n",
      "INFO:tensorflow:loss = 0.0538875, step = 501 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1146.14\n",
      "INFO:tensorflow:loss = 0.125988, step = 601 (0.087 sec)\n",
      "INFO:tensorflow:global_step/sec: 1301.04\n",
      "INFO:tensorflow:loss = 0.0534459, step = 701 (0.077 sec)\n",
      "INFO:tensorflow:global_step/sec: 1131.41\n",
      "INFO:tensorflow:loss = 0.045059, step = 801 (0.088 sec)\n",
      "INFO:tensorflow:global_step/sec: 1244.28\n",
      "INFO:tensorflow:loss = 0.0522664, step = 901 (0.080 sec)\n",
      "INFO:tensorflow:Saving checkpoints for 1000 into /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpxkfol9hw/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.0228446.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.estimator.estimator.Estimator at 0x1815a96ba8>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.train(\n",
    "    input_fn=lambda:train_input_fn(train_feature, train_label, batch_size = 32),\n",
    "    steps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2018-05-26-23:28:57\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from /var/folders/s2/z1q3239559s96vnyx0npgcqn4bw6wc/T/tmpxkfol9hw/model.ckpt-1000\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2018-05-26-23:28:57\n",
      "INFO:tensorflow:Saving dict for global step 1000: accuracy = 0.933333, global_step = 1000, loss = 0.0585234\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.93333334, 'global_step': 1000, 'loss': 0.058523417}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.evaluate(\n",
    "    input_fn=lambda:eval_input_fn(test_feature, test_label, batch_size = 32),\n",
    "    steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revisit Keras "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Activation, Dropout, Embedding, Input, Bidirectional, LSTM\n",
    "from numpy.random import seed\n",
    "seed(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dummy data\n",
    "import numpy as np\n",
    "data = np.random.random((1000, 100))  # num_examples, dimension\n",
    "labels = keras.utils.to_categorical(np.random.randint(10, size=(1000, 1)), num_classes=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 0s 472us/step - loss: 2.3312 - acc: 0.1030\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 51us/step - loss: 2.3045 - acc: 0.1140\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1818d8b048>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(32, activation='relu', input_dim=100))\n",
    "#model.add(Dense(32, activation='relu', input_shape=(100,)))\n",
    "model.add(Dense(10, activation='sigmoid'))\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model class with functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1000/1000 [==============================] - 0s 426us/step - loss: 2.3470 - acc: 0.1150\n",
      "Epoch 2/2\n",
      "1000/1000 [==============================] - 0s 38us/step - loss: 2.3168 - acc: 0.1220\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x181908e7f0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start from defining input and output\n",
    "\n",
    "inputs = Input(shape=(100,))\n",
    "\n",
    "activation = Dense(32, activation='relu')(inputs)\n",
    "predictions = Dense(10, activation='softmax')(activation)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Below is same\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model, iterating on the data in batches of 32 samples\n",
    "model.fit(data, labels, epochs=2, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.14737545,  0.10921454,  0.09013689,  0.08200558,  0.08720727,\n",
       "        0.05933505,  0.12864237,  0.08090284,  0.08317702,  0.13200295], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(data)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.        ,  1.16433334,  0.49090371,  0.67491192,\n",
       "        0.65327185,  0.30496395,  0.        ,  0.        ,  0.        ,\n",
       "        0.71922022,  0.49337339,  0.        ,  0.09041969,  0.        ,\n",
       "        1.60054719,  0.42954072,  0.        ,  0.43319386,  0.        ,\n",
       "        0.        ,  0.        ,  0.34784928,  0.        ,  0.        ,\n",
       "        0.        ,  0.        ,  0.82561976,  0.1011894 ,  0.        ,\n",
       "        0.        ,  0.        ], dtype=float32)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intermediate_layer_model = Model(inputs=inputs,\n",
    "                                 outputs=activation)\n",
    "intermediate_output = intermediate_layer_model.predict(data)\n",
    "intermediate_output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: LSTM and sentence classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dummy data\n",
    "- LSTM for learning sentence encoding\n",
    "- Dense layer for learning classification\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*GRQ91HNASB7MAJPTTlVvfw.jpeg\" width=\"400\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_size = 1000\n",
    "sequence_length = 100\n",
    "num_examples = 5000\n",
    "num_classes = 10\n",
    "batch_size = 2000\n",
    "embedding_dim = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(np.random.rand(num_examples, sequence_length) * vocabulary_size, dtype = np.int32)\n",
    "y_train = keras.utils.to_categorical(np.random.randint(num_classes, size=(num_examples, 1)), \n",
    "                                     num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = Embedding(vocabulary_size, embedding_dim, input_length = sequence_length)(inputs)\n",
    "all_h,last_h,last_c = LSTM(64,return_sequences=True, return_state=True)(embedding)\n",
    "dropout = Dropout(0.5)(last_h)\n",
    "output = Dense(num_classes, activation='softmax')(dropout)\n",
    "model = Model(inputs=inputs, outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4000 samples, validate on 1000 samples\n",
      "Epoch 1/1\n",
      "4000/4000 [==============================] - 4s 892us/step - loss: 2.3004 - acc: 0.1262 - val_loss: 2.3031 - val_acc: 0.0980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x182bcc5208>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile('adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=1,\n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How to understand `LSTM(64,return_sequences=True, return_state=True)` **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 64)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=inputs,outputs=all_h)\n",
    "y1 = model.predict(x_train[:5])[0]\n",
    "y1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(inputs=inputs,outputs=last_h)\n",
    "y2 = model.predict(x_train[:5])[0]\n",
    "y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(y1[99] == y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Take a look at `Bi-directional LSTM`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Tensor 'bidirectional_6/concat:0' shape=(?, ?, 128) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_6/while/Exit_2:0' shape=(?, 64) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_6/while/Exit_3:0' shape=(?, 64) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_6/while_1/Exit_2:0' shape=(?, 64) dtype=float32>,\n",
       " <tf.Tensor 'bidirectional_6/while_1/Exit_3:0' shape=(?, 64) dtype=float32>]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(sequence_length,))\n",
    "embedding = Embedding(vocabulary_size, embedding_dim, input_length = sequence_length)(inputs)\n",
    "all_outputs = Bidirectional(LSTM(64,return_sequences=True, return_state=True, ), \n",
    "                            merge_mode='concat')(embedding)\n",
    "concat_h, f_last_h,f_last_c, b_last_h, b_last_c = all_outputs\n",
    "all_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref: \n",
    "https://liufuyang.github.io/2017/04/01/just-another-tensorflow-beginner-guide-3.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "np.random.seed(123)\n",
    "tf.set_random_seed(123)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import mnist\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "batch_size = 128\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Convolution2D(32, (6, 6), activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Convolution2D(20, (6, 6), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 23, 23, 32)        1184      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 20)        23060     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 9, 9, 20)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 9, 9, 20)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1620)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               207488    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 233,022\n",
      "Trainable params: 233,022\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 95s 2ms/step - loss: 0.2440 - acc: 0.9248 - val_loss: 0.0484 - val_acc: 0.9834\n",
      "CPU times: user 6min 29s, sys: 1min 58s, total: 8min 27s\n",
      "Wall time: 1min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size, \n",
    "          epochs = 1, \n",
    "          verbose = 1,\n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.048365276794834065\n",
      "Test accuracy: 0.9834\n"
     ]
    }
   ],
   "source": [
    "model.save('./model/model_mnist_cnn.h5')\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
