{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Shilad_Sen/publication/221607721/figure/fig1/AS:305580804722702@1449867549680/Intermediary-entities-center-relate-user-to-recommended-item.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General Workflow**\n",
    "- Generate features from user behavior (indirect) / user attributes (direct) for pairs of $(u,i)$\n",
    "    - Behavior weighing (view, click, purchase)\n",
    "    - Behavior time (recent or past)\n",
    "    - Behavior frequency\n",
    "    - Item popularity (more popular, less important)\n",
    "    \n",
    "    \n",
    "- Apply algorithms (as in following sections), and get candidate list of items\n",
    "    - For a give user feature vector, retrieve $(item, weight)$\n",
    "    - There can be multiple tables (e,g., one table for CF, one-table for content-based) or (e.g., one table for browse, one table for click)\n",
    "\n",
    "\n",
    "- Filtering based on business rules\n",
    "    - For example, only recommend new products\n",
    "    \n",
    "    \n",
    "- Ranking by some criterias\n",
    "    - For example, variety\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data also includes:\n",
    "- User Info (sex, income)\n",
    "- Item Info (BOW, TF-IDF)\n",
    "- User-Item Interaction\n",
    "    - active/explicit: rating\n",
    "    - passive/implicit: clickstream analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory-based CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load some sample dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import svds\n",
    "from scipy.spatial.distance import cosine\n",
    "from scipy import sparse\n",
    "from script import cf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Import Example Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load example data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "header = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "df = pd.read_csv('./data/ml-100k/u.data', sep='\\t', names=header)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of users = 943 | Number of movies = 1682\n"
     ]
    }
   ],
   "source": [
    "n_users = df.user_id.unique().shape[0]\n",
    "n_items = df.item_id.unique().shape[0]\n",
    "print ('Number of users = ' + str(n_users) + ' | Number of movies = ' + str(n_items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_matrix = np.zeros((n_users, n_items))\n",
    "for line in df.itertuples():\n",
    "    df_matrix[line[1]-1, line[2]-1] = line[3]\n",
    "df_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Cosine Similarity\n",
    "$Similarity = cos(\\theta) = \\frac{\\mathbf A \\cdot \\mathbf B }{||\\mathbf A|| \\ ||\\mathbf B|| }$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_similarity = np.zeros((n_users, n_users))\n",
    "for i in range(n_users):\n",
    "    u= df_matrix[i,]\n",
    "    for j in range(n_users):\n",
    "        v= df_matrix[j,]\n",
    "        user_similarity[i,j] = np.dot(u,v) / (np.linalg.norm(u,ord=2) * np.linalg.norm(v,ord=2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 943)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import pairwise_distances\n",
    "user_similarity = pairwise_distances(df_matrix, metric='cosine')\n",
    "user_similarity.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 1682)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_similarity = pairwise_distances(df_matrix.T, metric='cosine')\n",
    "item_similarity.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Rating prediction: RMSE, MAE\n",
    "- Top-k rating precision: Precision, Recall, AUC\n",
    "    - % of the top-k recommendations that were actually used by user\n",
    "- Possible benchmark model: global top-k recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User-Item Filtering\n",
    "- Users who are similar to you also liked ...\n",
    "- Prediction for user __k__ for movie **m**:\n",
    "\n",
    "- Prediction = User **k** bias + adjustmemnt from similar user\n",
    "    - $ \\hat{x}_{k,m} = \\bar x_k + \\frac{\\Delta}{Norm}  $\n",
    "\n",
    "\n",
    "- Adjustment = (similarity with another user) * (rating of another user - bias of another user)\n",
    "    - $ \\Delta = \\sum_{k_0}UserSim(k, k_0) \\cdot (x_{k_0, m} - \\bar x_{k0})$\n",
    "\n",
    "    - $ Norm = \\sum_{k_0}|UserSim(k, k_0)|$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/cf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/cf.py\n",
    "import numpy as np\n",
    "\n",
    "def ui_predict(ratings, similarity):\n",
    "    all_user_mean = ratings.mean(axis = 1)\n",
    "    ratings_diff = (ratings - all_user_mean[:, np.newaxis]) # (943, 1682)\n",
    "    \n",
    "    adjust = similarity.dot(ratings_diff)\n",
    "    norm = np.array([np.abs(similarity).sum(axis=1)]).T\n",
    "\n",
    "    pred = all_user_mean[:, np.newaxis] + adjust / norm\n",
    "    \n",
    "    return pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from script.cf import *\n",
    "cf.ui_predict(df_matrix, user_similarity).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "### Item-Item Filtering\n",
    "Users who liked this item also liked ...\n",
    "- $ \\hat{x}_{k,m} = \\frac{\\sum_{m_0}ItemSim(m, m_0)}{Norm}  \\cdot x_{k, m_0}$\n",
    "   \n",
    "How to understand $ItemSim(m, m_0)$:\n",
    "- Dot product: number of users who like both item $m$ and item $m_0$ (If input matrix is not rating)\n",
    "- $ItemSim(m, m_0)$: added normalization to [0,1]\n",
    "\n",
    "Result:\n",
    "- Each item $m_0$ is contributing to rating of the target item $m$\n",
    "<img src=\"http://n.sinaimg.cn/sinacn23/279/w640h439/20180715/cbd7-hfkffak1630519.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/cf.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/cf.py\n",
    "\n",
    "def ii_predict(ratings, similarity):\n",
    "    norm = np.array([np.abs(similarity).sum(axis=1)])\n",
    "    pred = ratings.dot(similarity)  / norm\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 1682)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf.ii_predict(df_matrix, item_similarity).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison between user-based and item-based\n",
    "|User-based       | Item-based           | \n",
    "| ------------- |:-------------:| \n",
    "| more socialized      | more personalized | \n",
    "| fro example: news | for example: books|\n",
    "| update user-similarity matrix| update item-similarity matrix|\n",
    "| number of user << number of items | number of user >> number of items|\n",
    "| Not interpretable | Interpretable|\n",
    "| Cold starting: No problem for new items (after 1 action) | Cold starting: no problem for new users (after one action)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based CF - Matrix Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Factorization or Latent Factor Model\n",
    "Singular-Value-Decomposition\n",
    "\n",
    "- see SVD notebook for math behind SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://sigopt.com/wp-content/uploads/2018/10/collaborative_filtering.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $M = U \\times \\Sigma \\times V^T  $ Note: $\\Sigma$ can be multipled to U or V\n",
    "- Con: need default value for missing value in rating matrix\n",
    "    - For like (0/1) or implicit feedback: \n",
    "    - 1) balanaced negative samples + postive samples\n",
    "    - 2) draw negative samples from popular items \n",
    "    \n",
    "    \n",
    "- U and V are low-dimention latent vectors (Embeddings) for users and movies. \n",
    "    - How to interpret this dimensions? Genres (i.e., users' preference for Genre $k$ and a given book's weight on genre $k$)\n",
    "    \n",
    "    \n",
    "- Alternative approach: $Min(L) = \\sum_{i,j}{(u_i v_j - x_{ij})^2} + Regularization (u,v)$\n",
    "    - Solved by SGD\n",
    "   \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "u, s, vt = svds(df_matrix, k = 20)\n",
    "s_diag_matrix = np.diag(s)\n",
    "X_pred = np.dot(np.dot(u, s_diag_matrix), vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://cdn-images-1.medium.com/max/1760/1*2i-GJO7JX0Yz6498jUvhEg.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other methods: \n",
    "- Probabilistic factorization (PMF)\n",
    "    * P = dot_product(i,j) + User_i_bias + movie_j_bias + intercept\n",
    "- Non-negative factorization (NMF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Weighted rankings from two models\n",
    "- Cascade: 1) Accurate model -> rough rank; 2) 2nd one to refine\n",
    "- Treat collaborative factors as extra feature for content-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with memory-based CF\n",
    "- No need to store user-user ($ U \\times U $) or Item-Item ($ I \\times I$) similarity matrix in memory. The memory requirement is $F \\times (U + I)$\n",
    "- Matrix calculation on real-time is hard for large number of items, so mainly for off-line. Time complexity can be $F \\times U \\times I$. While memory-based method just needs a look-up table.\n",
    "- Hard to interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model-Based CF - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main difference with SVD\n",
    "    - Don't require orthogonal vectors as in SVD\n",
    "    - Learn the embedding by itself\n",
    "    - Allows non-linearity instead of just dot product\n",
    "    \n",
    "    \n",
    "- Extra benefits\n",
    "    - Can incoporate additional features like user profile\n",
    "    \n",
    "\n",
    "## Model Types\n",
    "\n",
    "### Idea 1\n",
    "- One-hot encoding for user i\n",
    "- Hiddern layer: Embedding layer for users\n",
    "    - Weights: latent vector for users\n",
    "    \n",
    "    \n",
    "- Output layer: output ratings for each item j\n",
    "    - Weights: latent vector for movies\n",
    "    \n",
    "    \n",
    "###  Idea 2\n",
    "- One-hot encoding for user i\n",
    "- Hiddern layer: Embedding layer for users\n",
    "    - Weights: latent vector for users\n",
    "    \n",
    "    \n",
    "- One-hot encoding for item j\n",
    "- Hidden layer: output ratings for each movie j\n",
    "    - Weights: latent vector for movies\n",
    "\n",
    "\n",
    "- More hidden layers\n",
    "    - Compared with matrix factorization, more representation power\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/neuralcollaborativefiltering-170528094418/95/neural-collaborative-filtering-9-638.jpg?cb=1496201763\" width=\"600\">\n",
    "    \n",
    "\n",
    "###  A lot of other DL network types    \n",
    "\n",
    "Example: https://arxiv.org/abs/1708.05031\n",
    "<img src=\"https://nipunbatra.github.io/blog/2017/neumf.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Neural CF in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from IPython.display import SVG\n",
    "from keras import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Input, Embedding, Flatten, Dot, Concatenate, Dense, Dropout\n",
    "from keras.utils.vis_utils import model_to_dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_latent_factors = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_input = Input(shape = [1], name = 'Item-input')\n",
    "item_embedding = Embedding(n_items, n_latent_factors, name = 'Item-embedding')(item_input)\n",
    "item_flat = Flatten(name = 'Item-flatten')(item_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = Input(shape = [1], name = 'User-input')\n",
    "user_embedding = Embedding(n_users, n_latent_factors, name = 'User-embedding')(user_input)\n",
    "user_flat = Flatten(name = 'User-flatten')(user_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "dotprod = Dot(axes=1, name='DotProduct')([item_flat, user_flat])\n",
    "model = Model([item_input, user_input ], dotprod)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='./figure/model_ncf.png')\n",
    "#SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/model_ncf.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "concatenate = Concatenate(name='Concatenate')([item_flat, user_flat])\n",
    "dropout = Dropout(0.2,name='Dropout')(concatenate)\n",
    "dense_1 = Dense(20,activation='relu', name='FC-1')(dropout)\n",
    "activation = Dense(1, activation='relu', name='Activation')(dense_1)\n",
    "model = Model([item_input, user_input ], activation)\n",
    "model.compile('adam', 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='./figure/model_ncf2.png')\n",
    "#SVG(model_to_dot(model,  show_shapes=True, show_layer_names=True, rankdir='HB').create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/model_ncf2.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 80000 samples, validate on 20000 samples\n",
      "Epoch 1/15\n",
      " - 4s - loss: 1.5278 - val_loss: 0.9136\n",
      "Epoch 2/15\n",
      " - 3s - loss: 0.9126 - val_loss: 0.8956\n",
      "Epoch 3/15\n",
      " - 3s - loss: 0.8964 - val_loss: 0.8882\n",
      "Epoch 4/15\n",
      " - 3s - loss: 0.8809 - val_loss: 0.8799\n",
      "Epoch 5/15\n",
      " - 3s - loss: 0.8614 - val_loss: 0.8652\n",
      "Epoch 6/15\n",
      " - 3s - loss: 0.8486 - val_loss: 0.8639\n",
      "Epoch 7/15\n",
      " - 3s - loss: 0.8381 - val_loss: 0.8644\n",
      "Epoch 8/15\n",
      " - 3s - loss: 0.8255 - val_loss: 0.8608\n",
      "Epoch 9/15\n",
      " - 3s - loss: 0.8156 - val_loss: 0.8549\n",
      "Epoch 10/15\n",
      " - 3s - loss: 0.8033 - val_loss: 0.8568\n",
      "Epoch 11/15\n",
      " - 3s - loss: 0.7958 - val_loss: 0.8511\n",
      "Epoch 12/15\n",
      " - 3s - loss: 0.7857 - val_loss: 0.8489\n",
      "Epoch 13/15\n",
      " - 3s - loss: 0.7771 - val_loss: 0.8513\n",
      "Epoch 14/15\n",
      " - 3s - loss: 0.7703 - val_loss: 0.8451\n",
      "Epoch 15/15\n",
      " - 3s - loss: 0.7656 - val_loss: 0.8548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18198a7898>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://keras.io/models/model/\n",
    "model.fit(x = [df.item_id - 1, df.user_id - 1], \n",
    "          y = df.rating, \n",
    "          epochs = 15, \n",
    "          verbose = 2, \n",
    "          validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(943, 20)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(name='User-embedding').get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1682, 20)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_layer(name='Item-embedding').get_weights()[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-basd Recommendation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Different from item-based CF, where similarity is calculated based on user-item-interactions\n",
    "- Calculate **similarity** with **items** of **ONE user**\n",
    "- Similarity is calculated based on item attribute (for example, location, price, cuisine, etc.)\n",
    "    - Output: An item space with defined distance\n",
    "- One model for one user; No interaction between users\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Ranking Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Top-K ranking problem instead of rating prediction problem\n",
    "- More consistent with practical user needs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to evaluate a ranking system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recall and Precision\n",
    "- Given user $u$, the model generates a recommendation set $R$ and true set (where the user likes/rates) $T$\n",
    "- $Recall = \\frac{R \\cap T}{T}$\n",
    "- $Precision = \\frac{R \\cap T}{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AP (Average Precision)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Average Precision = \\frac{\\sum_{k=1}^{N}{P(k) * rel(k)}}{K} $\n",
    "- $P(k)$ is precision of first k results\n",
    "- $rel(k)$ is binary value 0/1\n",
    "- $K$ is total number of relevant items"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://slideplayer.com/2295316/8/images/4/Mean+Average+Precision.jpg\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Discounted Cumulative Gain (NDCG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The premise of DCG is that highly relevant documents appearing lower in a search result list should be penalized as the graded relevance value is reduced logarithmically proportional to the position of the result.\n",
    "- Note: rel(i) here can be any value instead of binary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://image.slidesharecdn.com/colmujsktqk4sh7faxcd-signature-f4a0831a458d6bbb78c09b1a397c3517fe8a2c82e9751f039f832a3be97b108f-poli-141208101339-conversion-gate02/95/florian-douetteau-dataiku-7-638.jpg?cb=1418033719\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Discounted_cumulative_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage\n",
    "<img src=\"https://slideplayer.com/10441443/35/images/7/Coverage+Measure+the+ability+of+recommender+system+to+recommend+all+items+to+users.+Entropy%2C+Gini+Index..jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/divers-111026095821-phpapp01/95/towards-diverse-recommendation-72-728.jpg?cb=1319623189\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other metrics:\n",
    "Diversity, AUC, F1, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pointwise\n",
    "    - Given user u and item i, predict score x\n",
    "    - Not necessary, since the score is not important; ranked list is important.\n",
    "    - Actually solving a *regression* problem\n",
    "    - The relationships between documents sometimes not considered\n",
    "    - Usually: regression, classification, etc\n",
    "    \n",
    "<img src=\"https://image.slidesharecdn.com/l2rrecysystutaly-final-131012040539-phpapp01/95/learning-to-rank-for-recommender-systems-acm-recsys-2013-tutorial-40-638.jpg?cb=1381555055\" width=\"400\">\n",
    "    \n",
    "    \n",
    "- Pairwise\n",
    "    - Goal is to correctly determine a>b or a<b for each document pair\n",
    "    - The scale of difference is ignored\n",
    "\n",
    "<img src=\"https://image.slidesharecdn.com/l2rrecysystutaly-final-131012040539-phpapp01/95/learning-to-rank-for-recommender-systems-acm-recsys-2013-tutorial-46-638.jpg?cb=1381555055\" width=\"400\">\n",
    "\n",
    "- Listwise\n",
    "    - Directly optimize final performance metric\n",
    "    - More complicated modelling\n",
    "    \n",
    "<img src=\"http://baogege.info/img/learning-to-rank-for-recommender-system/listwiseltrrs.png\" width=\"400\">    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CTR (Click-Through Rate) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given (user, item, context): predict click = 0/1\n",
    "- Goal: $P(Click=1|User, Item, Context)$\n",
    "- Difference: in recommendation system and in online ad. \n",
    "- The former mainly cares about ranking, while the latter cares about accuracy, because it contributes to revenue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach - Traditional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Categorical features\n",
    "    - Millions of dimensions after one-hot encoding\n",
    "\n",
    "\n",
    "\n",
    "### Logistic Regression + Feature Engineering (Linear Model)\n",
    "- Advantage: simple model, good at dealing with discrete features\n",
    "- Advantage: linear model, can be parallelized\n",
    "- Main problem: feature combination, high-order interaction, only linear relationship --> (**Fix**: FM)\n",
    "- For example: gender and clothes type; manual interaction is required\n",
    "- Extension: MLR\n",
    "\n",
    "\n",
    "### GBDT\n",
    "- Advantage: good at dealing with continous features\n",
    "- Capable of doing some feature combination (more than 2 orders)\n",
    "- Capable of doing some feature selection\n",
    "- Again, like LR, cannot be genealized well\n",
    "\n",
    "### Degree-2 Polynomial Mappings \n",
    "- Combine features by $y(X)=w_0 + \\sum_{i=1}^{N}{w_{1i}x_i} + \\sum_{i=1}^{N}\\sum_{j=i+1}^{N}{w_{2ij}x_ix_j} $, where N is number of features\n",
    "- Sparse data (Cannot solve if there is even no $x_i = x_j = 0$ for some $i, j$)\n",
    "- High dimension: $O(n^2)$\n",
    "- Make trivial prediction on those unseen pairs\n",
    "    \n",
    "<img src=\"http://ailab.criteo.com/wp-content/uploads/2017/03/Screen-Shot-2017-02-10-at-11.12.46-AM-789x121.png\" width=\"400\">   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorization Machine (FM)\n",
    "- Known: for matrix $W$, and a large $K$, $W \\approx \\mathbf V\\mathbf V^T$, i.e., $w_{ij} \\approx <\\mathbf v_i, \\mathbf v_j>$ \n",
    "-  $y(\\mathbf x)=w_0 + \\sum_{i=1}^{N}{w_{1i}x_i} + \\sum_{i=1}^{N}\\sum_{j=i+1}^{N}{<\\mathbf v_i, \\mathbf v_j>x_ix_j} $\n",
    "- $\\mathbf v_i, \\mathbf v_j - R^{N \\times K}$, latent vector for feature $i$ and $j$ with embedding length $K$, and totally $N$ features\n",
    "    - $\\mathbf v_i = (v_{i,1}, v_{i,2},...,v_{i,f},...)$, where $i$ is index for feature, $f$ is index for feature space\n",
    "    - $\\frac{\\partial y}{\\partial v_{i,f}} = x_i \\sum_{j=1}^N v_{j,f}x_j-v_{i,f}x_i^2$. \n",
    "    - Intuively, gradient direction is towards $v_{j,f}$ wherever $x_i$ and $x_j$ are both one.\n",
    "- Some advantages:\n",
    "    - Reasonable prediction for unseen pairs\n",
    "    - Lower dimension compared with polynomial: $O(N \\times K)$\n",
    "\n",
    "<img src=\"http://ailab.criteo.com/wp-content/uploads/2017/03/Screen-Shot-2017-02-10-at-11.12.53-AM-768x301.png\" width=\"400\">   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Field Factorization Machine (FFM)\n",
    "- Split the original latent space into many “smaller” latent spaces,  and depending on the fields of features, one of them is used.\n",
    "- For example: weather, location, gender; intuitively, they should have different interactions\n",
    "- $y(X)=w_0 + \\sum_{i=1}^{N}{w_{1i}x_i} + \\sum_{i=1}^{N}\\sum_{j=i+1}^{N}{<\\mathbf v_{i, f_j}, \\mathbf v_{j, f_i}>x_ix_j} $\n",
    "<img src=\"http://ailab.criteo.com/wp-content/uploads/2017/03/Screen-Shot-2017-02-10-at-11.13.03-AM-768x230.png\" width=\"400\"> \n",
    "- Higher dimension: $O(N \\times K \\times F)$, where $F$ is number of fields\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBDT + LR (Mixed)\n",
    "- **Motivation**: GBDT transforms features, reduced dimensions, combined attributes\n",
    "- The leaves serve as new input features for LR\n",
    "- Drawback:\n",
    "    - Tree not very good for high-dimension sparse features \n",
    "    - Trees tend to overfitting because penalty is imposed on tree nodes/depth while LR penalizes weights\n",
    "- Alternative: \n",
    "    - Use GBDT for continous variables, get $gbdt(X_{continuous})$\n",
    "    - Concatenate with $X_{discrete}$ to get final $\\mathbf X$, and feed into LR.\n",
    "\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/PnYuan/Practice-of-Machine-Learning/master/imgs/Kaggle_CTR/gbdt-lr/gbdt-lr_2.png\" width=\"500\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach - Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Main Advantage\n",
    "    - High-order interaction is possible by nature by hidden layers\n",
    "    - Enabled interaction of more than two (>2) features\n",
    "    - Low-order is captured by shallow part\n",
    "    - Can be extended into figures/texts\n",
    "    \n",
    "    \n",
    "- Main characteristic\n",
    "    - Shallow part of model\n",
    "    - Stack part of model\n",
    "        - Concentenate\n",
    "        - Bi-Interaction\n",
    "    - Start: Embedding layer\n",
    "    - End: FC layer\n",
    "    \n",
    "<img src=\"https://pic2.zhimg.com/80/v2-627974c0bb215cb6404e9ee51bbb5752_hd.jpg\" width=\"700\">\n",
    "\n",
    "ref: https://github.com/hhlisme/Daily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Describe Factorization Machine (FM) as deep learning network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/d-fm.png\" width = \"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wide and Deep\n",
    "- Memorization: learning the directly relevant frequent co-occurrence of items;\n",
    "- Generalization: improving the diversity exploring new items combinations that have never or rarely occurred in the past.\n",
    "- **Wide/Shallow**: LR for categorical variables --> Memorization\n",
    "- **Deep/Stack**: DNN for continous/categorical variables --> Generalization\n",
    "<img src=\"https://1.bp.blogspot.com/-Dw1mB9am1l8/V3MgtOzp3uI/AAAAAAAABGs/mP-3nZQCjWwdk6qCa5WraSpK8A7rSPj3ACLcB/s1600/image04.png\" width=\"800\">\n",
    "---\n",
    "<img src=\"http://edarchimbaud.github.io/img/2016-11-22-wide-and-deep-learning-for-recommender-systems/Screenshot%20from%202016-11-22%2021-07-22.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An example from https://zhuanlan.zhihu.com/p/37823302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://pic4.zhimg.com/80/v2-e440f72e8bf2a4be2274235d481d4f63_hd.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Network structure**\n",
    "\n",
    "Part 4: Wide Part, including position bias, some categorical features to enable memorization\n",
    "\n",
    "Part 1: Deep Part, including categorical features\n",
    "- User ID / Item ID / Area ID\n",
    "- Some discretized continuous features\n",
    "- Missing / Lower frequency IDs treated as separate value\n",
    "\n",
    "Part 2: Deep Part, including continuous features\n",
    "- Some statistics\n",
    "- Text\n",
    "- Image\n",
    "\n",
    "Part 3: FC, Activation layer, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep FM\n",
    "- Ref: https://www.ijcai.org/proceedings/2017/0239.pdf\n",
    "- FM and Deep parts shares the same inputs; In wide&deep, the two parts are independent\n",
    "- Compared with wide&deep based on manually created features, Deep FM contains the inner product of feature latent vectors (automatically)\n",
    "- Network structure\n",
    "    - Directly from very sparse layer: zero and first order terms\n",
    "    - Very sparse -> Dense layer: \n",
    "        - from the neuron of $i^{th}$ feature: $\\mathbf v_i$ = $[v_{i1},...,v_{i,f}]$ for $i^{th}$ feature becomes the network weights ($V_i$ in the figure below).\n",
    "        - from a field of neurons: their embedding shares the same set of $f$\n",
    "        - One solution for multi-value field (for example: likes both apple and banana), then take average of two densed vectors (i.e., ***Average Pooling***)<img src=\"./figure/embed.png\" width=\"500\">\n",
    "         \n",
    "    - FM Layer, captures the lower (2) order combinations: Red arrow means 1 weight: $<\\mathbf v_i, \\mathbf v_j>x_ix_j $\n",
    "    - Deep/Stack layers: captures the higher order combinations, separate from FM\n",
    "    - $y_{DeepFm}=sigmoid(y_{FM}+y_{DNN})$\n",
    "\n",
    "<img src=\"./figure/deepfm.png\" width=\"600\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NFFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ref: https://arxiv.org/pdf/1904.12579.pdf\n",
    "\n",
    "\n",
    "<img src = \"https://deepctr-doc.readthedocs.io/en/latest/_images/NFFM.png\" width=\"450\">\n",
    "<img src = \"./figure/nffm.png\" width=\"350\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Factorization Machines (NFM)\n",
    "\n",
    "-  Combine features by $y(X)=w_0 + \\sum_{i=1}^{N}{w_{1i}x_i} + f(x)$, where N is number of features\n",
    "- Define Bi-interaction layer: $f_{BI}\\mathbf (V) = \\sum_i \\sum_j x_i v_i \\times x_j v_j$ (i.e., sum pooling with an output of $K$ dimension vector)\n",
    "\n",
    "- Below is the deep part of NFM (i.e, $f(x)$ term). It needs to be concatenated with 0 and 1 order terms (i.e., $wx + b)$ as the final output $\\hat y$.\n",
    "\n",
    "Difference with Deep FM:\n",
    "\n",
    "- In DeepFM: \n",
    "    - Low order interactions from FM **AND** higher order interactions from DNN\n",
    "    - Concatenation increases number of parameters, while here bi-interaction reduces complexity\n",
    "    \n",
    "    \n",
    "- In NFM: \n",
    "    - Low order interactions from FM **THEN** higher order interactions from DNN\n",
    "    - Sum of element-wise product loses informaion, but reduced parameters\n",
    "\n",
    "|Type|Characteristic|Example|\n",
    "|:-:|:-:|:-:|\n",
    "|Type1|FM and DNN are separately calculated, and then concatenated|DeepFM，DCN，Wide&Deep|\n",
    "|Type2|The 1st and 2nd order calculation of FM is used as input for DNN|PNN,NFM,AFM|\n",
    "\n",
    "---\n",
    "\n",
    "<img src = \"./figure/nfm.png\" width = \"700\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DIN (Deep Interest Network)\n",
    "- Main advantage:\n",
    "    - Considers **diversity** of user interest instead of embedding user history in the same way regardless of given candidate item\n",
    "    - **Local activation** helps linking only part of user's history with the candidate item (e,g., swimming cap - goggle). \n",
    "    - In other words, a weighted average pooling for user behavior history vectors.\n",
    "    \n",
    "    \n",
    "- Example of feature input\n",
    "<img src=\"./figure/table.png\" width=\"500\">\n",
    "<img src=\"./figure/feature_input\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Comparison with traditional network\n",
    "    - Key formula: $v_U(A) = f(v_A, e_1, e_2, ..., e_H) = \\sum_{j=1}^H f(e_j, v_A) e_j = \\sum_{j=1}^H w_je_j$, where $A$ is candidiate item id, $j$ is user history item id.\n",
    "<img src=\"./figure/two model 2.png\" width=\"600\">\n",
    "<img src=\"./figure/two models.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entire Space Multitask Model (ESMM)\n",
    "- User behavior: Impression -> Click -> Buy\n",
    "- CVR, CTR, CTCVR <img src=\"./figure/ctcvr.png\" width=\"400\">\n",
    "- Goal: address 1) sample selection bias and 2) data sparsity\n",
    "- Network structrure: \n",
    "    - In ESMM, two auxiliary tasks of CTR and CTCVR are\n",
    "introduced which: i) help to model CVR over entire input\n",
    "space, ii) provide feature representation transfer learning.\n",
    "    - Embedding parameters of CTR and CVR network\n",
    "are shared. CTCVR takes the product of outputs from\n",
    "CTR and CVR network as the output.\n",
    "<img src=\"./figure/essm.png\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Network\n",
    "- DCN\n",
    "- PNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling Approach - Reinforced Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "*Zheng, Guanjie, et al. \"DRN: A Deep Reinforcement Learning Framework for News Recommendation.\" Proceedings of the 2018 World Wide Web Conference on World Wide Web. International World Wide Web Conferences Steering Committee, 2018.*\n",
    "\n",
    "- Main advantage\n",
    "    - Dynamic nature of user interest\n",
    "    - Combination of short-term and long-term reward\n",
    "- State \n",
    "    - user features: statistics of news that the user clicked in the last hour/day\n",
    "    - context: time of day, etc.\n",
    "- Action\n",
    "    - news features: provider, topic, history clicks\n",
    "    - user-news interaction: e.g., the frequency of the type of news $i$ clicked by user $u$\n",
    "- Agent\n",
    "    - provide top-K list of news\n",
    "- Reward\n",
    "    - news click (short-term)\n",
    "    - user activeness (long-term)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/www2018-3-fig2.png\" width = \"500\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"http://deliveryimages.acm.org/10.1145/3190000/3185994/images/www2018-3-fig4.jpg\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "*Zhao, Xiangyu, et al. \"Deep Reinforcement Learning for List-wise Recommendations.\" arXiv preprint arXiv:1801.00209 (2017).*\n",
    "- Network structure comparison\n",
    "    - a) Cannot fit high action space (e.g., recommendation system)\n",
    "    - b) The network computes Q value for each action, separately, increasing time complexity\n",
    "    - c) Actor-Critic algorithm (see benefits in `Concept Notes.ipynb`)\n",
    "    \n",
    "- Action\n",
    "    - recommenda $K$ items instead of one\n",
    "<img src=\"./figure/DQN.png\" width=\"800\">  \n",
    "<img src=\"./figure/Actor-Critic.png\" width=\"600\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some feature engineering issues\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**General**\n",
    "- For continuous variables\n",
    "    - standardize for NN inputs\n",
    "    - discretize for LR\n",
    "    \n",
    "    \n",
    "    \n",
    "- For discrete variables\n",
    "    - One-got encoding\n",
    "    - Text: BoW(n-gram), TF-IDF\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**List/Type of features**\n",
    "- User Characteristic\n",
    "    - Demographic\n",
    "    - Behavior, preference, activeness\n",
    "- Business Characteristics\n",
    "    - Type, city, star, etc\n",
    "    - Image\n",
    "- Query\n",
    "    - Tokens, similarity\n",
    "- Context\n",
    "    - Time, distance, competition\n",
    "    - Position bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key feature: user behavior**\n",
    "- Real time user behavior\n",
    "    - Clicked Business (C_P)\n",
    "    - Ordered Business (O_P)\n",
    "    - Queries (Q)\n",
    "    - Sortings (S)\n",
    "    - Business Characteristic (C_Type, O_Type, C_Loc, O_Loc)\n",
    "    \n",
    "    \n",
    "- Problem: Sparsity of C_P, O_P, Q, S\n",
    "- Fix: separate model to describe **USER** based on user behavior \n",
    "    - Predict next time t user behavior based on LTSM\n",
    "    - Doc2Vec to get embedding of a user based on behaviors\n",
    "    - Topic modelling\n",
    "    - Serve as continous feature in Part 2\n",
    "\n",
    "\n",
    "**Some examples of extra features**\n",
    "- Statistics for different types of behaviors, for example, conversion rate, device, etc.\n",
    "    - Counting features like device ip count, device id count, hourly user count, user count\n",
    "    - Bagging features\n",
    "        - user|app id|bag of app id\n",
    "        - user1| A |A, B\n",
    "        - user1| B |A, B\n",
    "        - user2| C |C, D\n",
    "        - user2 |D |C, D\n",
    "    - Click history\n",
    "        - label| user |history\n",
    "        - 0 |user1|\n",
    "        - 1 |user1 |0\n",
    "        - 1 |user1 |01\n",
    "        - 0 |user1 |011\n",
    "- Paattern/Series of behaviors (A-B-C)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cold Starting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For new users:\n",
    "- Provide most popular items\n",
    "- Provide results based on demographic attributes (gender, age, etc)\n",
    "- Ask users to provide feedback on some items before providing recommendation\n",
    "\n",
    "\n",
    "For new items:\n",
    "- If using user-based CF, then as long as some user finds the items from other sources, it will be spread among users\n",
    "- If using item-based CF, then have to use item content to calculate item similarity, otherwise it will never be presented to users\n",
    "    - Calculate item-similarity by vectorizing items\n",
    "    - A strong feature would actually help content-based recommendation out-perform CF\n",
    "    \n",
    "- Some examples of similarity w/o user behavior history\n",
    "    - keyword vector of an item title with the help of NER\n",
    "    - TF-IDF of a text\n",
    "    - LDA topic modelling (use topic vector to calculate similarity)\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Reference\n",
    "- https://arxiv.org/pdf/1606.07792.pdf\n",
    "- https://www.slideshare.net/kerveros99/deep-learning-for-recommender-systems-recsys2017-tutorial?from_action=save\n",
    "- https://medium.com/recombee-blog/machine-learning-for-recommender-systems-part-1-algorithms-evaluation-and-cold-start-6f696683d0ed\n",
    "- https://www.slideshare.net/microlife/recommender-systems-contentbased-and-collaborative-filtering\n",
    "- https://www.slideshare.net/LadislavPeska/towards-recommender-systems-for-police-photo-lineup?next_slideshow=1\n",
    "- https://nycdatascience.com/blog/student-works/capstone/metarecommendr-recommendation-system-video-games-movies-tv-shows/\n",
    "- https://www.researchgate.net/publication/317558508_Item_Silk_Road_Recommending_Items_from_Information_Domains_to_Social_Users?enrichId=rgreq-ea35906fbd73392cb2fd6212acddf950-XXX&enrichSource=Y292ZXJQYWdlOzMxNzU1ODUwODtBUzo2MDM1NDY3NDc2ODY5MThAMTUyMDkwODE2NjY0OQ%3D%3D&el=1_x_3&_esc=publicationCoverPdf\n",
    "- https://www.jianshu.com/p/005a4e6ac775\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://pnyuan.github.io/blog/ml_practice/Kaggle%E6%BB%91%E6%B0%B4%20-%20CTR%E9%A2%84%E4%BC%B0%EF%BC%88GBDT-LR%EF%BC%89/\n",
    "- http://ailab.criteo.com/ctr-prediction-linear-model-field-aware-factorization-machines/\n",
    "- http://kubicode.me/2018/02/23/Deep%20Learning/Deep-in-out-Factorization-Machines-Series/\n",
    "- https://zhuanlan.zhihu.com/p/37823302"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Wangyi Music Recommendation): https://mp.weixin.qq.com/s/1YUtk3It574jIBMxbkhmpg "
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "632px",
    "left": "0px",
    "right": "1261.17px",
    "top": "106.992px",
    "width": "183.857px"
   },
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
