{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some key concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "- feature engineering and SVM\n",
    "- conputational speed and GPU\n",
    "- dataset size and internet, orders of magnitude more data\n",
    "- Framework such as TF, Keras\n",
    "\n",
    "**NN as universal approximators**\n",
    "- More neurons --> more complicated functions\n",
    "- Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Activation Function**\n",
    "- Sigmoid\n",
    "    * Saturated at 0/1 and kills gradients (derivative -> 0)\n",
    "    * Output not zero-centred; for next layer: f = wx + b, x>0, df/dw same sign for all w; zig-zag update trajectory\n",
    "    \n",
    "    \n",
    "- Tanh\n",
    "    * Still kills gradients\n",
    "    * But: zero-cented\n",
    "    \n",
    "    \n",
    "- ReLU\n",
    "    * Non-saturated, linearity --> Accelerate convergence\n",
    "    * Cheap computation\n",
    "    * But: Can die; never activate\n",
    "    * Extension: Leaky ReLU, maxout\n",
    "\n",
    "\n",
    "- Leaky ReLU\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"500\">\n",
    "\n",
    "\n",
    "## **Regularization**\n",
    "- L1/L2/ElasticNet\n",
    "- Max Norm constraint\n",
    "- Drop Out layer\n",
    "\n",
    "## **Hyperparameter Optimization**\n",
    "- Single validation set > cross validation in practice\n",
    "- Random search instead of grid search within a range\n",
    "- Metric selection: accuracy, RMSE, etc.\n",
    "- Ideally: Training + Validation + Test, where validation set is to *\"learn\"* hyperparameters. \n",
    "    - Think of hyperparam tuning itself a learning algorithm and validation set becomes \"training set\" in the new problem.\n",
    "\n",
    "\n",
    "- Important hyperparams:\n",
    "    - Network structure\n",
    "    - Batch size\n",
    "    - Learing rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Weights Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **All zero**: \n",
    "    - Wrong: neuron outputs and gradients would be same; same update\n",
    "    - We don't want the symmetric structure of weights\n",
    "    - The model will be equivalent to linear model since each weight *w* has the same update\n",
    "    \n",
    "    \n",
    "- **Number to small**: \n",
    "    - small gradients for hidden layer inputs ($\\frac{\\partial L}{\\partial h} = W$)\n",
    "    - ***Gradient Vanishing*** when flowing backwafrd, leading to convergence of the cost before it has reached the minimum value.\n",
    "\n",
    "\n",
    "- **Number to big**: \n",
    "    - ***Gradient Exploding*** problem may happen, and cause the cost to oscillate around its minimum value.\n",
    "    - will also be a problem is activation functions like sigmoid is used since the **WX** may fall into zero-gradient region\n",
    "\n",
    "\n",
    "- **Preferred: All neuron with a good output distribution to feed next layer**:\n",
    "    - Benefit: effective back-propagation for weights\n",
    "    - w = np.random.randn(n) / sqrt(n), where n is number of inputs. In other words, the standard error $std(w) = \\sqrt\\frac {1}{n}$\n",
    "    - It can be proved that Var(S) = Var(WX) = Var(X)\n",
    "    - It helps get a wide range of output value for each hidden layer\n",
    "    - For activation functions ReLU, see below: $std(w) = \\sqrt\\frac {2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ***To sum up, the goal is to***:\n",
    "    - The mean of the activations should be zero.\n",
    "    - The variance of the activations should stay the same across every layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **An example**\n",
    "    - An example shows how weight initialization helps keep the same distribution of each layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "node_num = 100    \n",
    "    \n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def get_activation(w, activation):\n",
    "    x = np.random.randn(10000, 100) \n",
    "    hidden_layer_size = 5\n",
    "    activations = {}\n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x = activations[i-1]\n",
    "        z = np.dot(x, w)\n",
    "        if activation == \"sigmoid\":\n",
    "            a = sigmoid(z)\n",
    "        elif activation == \"ReLU\":\n",
    "            a = ReLU(z)\n",
    "        activations[i] = a\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.randn(node_num, node_num) * 1 / np.sqrt(node_num)\n",
    "activations = get_activation(w, 'sigmoid')\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(len(activations),2 , 2 * i + 1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "    \n",
    "w = np.random.randn(node_num, node_num) * 2 / np.sqrt(node_num)\n",
    "activations = get_activation(w, 'ReLU')    \n",
    "for i, a in activations.items():\n",
    "    plt.subplot(len(activations),2 , 2 + 2 * i)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0.01,1))   \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce interaction/co-adapt between units -> better generalization\n",
    "- Similar to subsampling/bagging on network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/dropout.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Train:\n",
    "    - `self.mask = np.random.rand(*self.W.shape) < self.p / self.p`\n",
    "    - `self.W *= self.mask`\n",
    "\n",
    "- Test:\n",
    "    - As usual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Normalization**\n",
    "###  Internal Covariate Shift\n",
    "- Different layers have different distributions of input data\n",
    "    \n",
    "    - 1. Gradient Vanishing: activation function input value within nonlinear regime for sigmoid function <img src=\"https://image.slidesharecdn.com/dlmmdcud1l06optimization-170427160940/95/optimizing-deep-networks-d1l6-insightdcu-machine-learning-workshop-2017-8-638.jpg?cb=1493309658\" width=\"500\">\n",
    "\n",
    "    - 2. Slow learning: different scale makes it harder for faster convergence using SGD\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/FeatureScaling.jpg\" width=\"500\">\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "1. **Shift and scale the output values to $N(0,1)$**\n",
    "    - <img src=\"https://guillaumebrg.files.wordpress.com/2016/02/bn.png?w=656\" width=\"500\">\n",
    "\n",
    "    - Improve gradient flow (point1)\n",
    "    - Allow higher learning rates (point2)\n",
    "    - Reduce dependence on initialization (output distribution no long depends on weights $W$)\n",
    "    - *Note*: At test time, the mean from training should be used instead of calculated from testing batch\n",
    "\n",
    "\n",
    "2. **Learn $\\gamma$ and $\\beta$ to retrieve representation power**\n",
    "    - The distribution of the feature may be located at two sides of the non-linear regions for sigmoid funciton, and forcing it to be standardized will lose the distribution.\n",
    "    \n",
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" width=\"400\">\n",
    "\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-36a6ee9c550f479fc681eab380510baf\" width=\"500\">\n",
    "\n",
    "<img src=\"https://forums.fast.ai/uploads/default/original/2X/9/998a1be6463260f731481106756034c42040e256.jpg\" width=\"400\">\n",
    "\n",
    "3. **How to predict**\n",
    "    - Use the unbiased estimation of mean and variance from all train batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Param Update and Learning Rate**\n",
    "- Step decay for learning rate: \n",
    "    * Reduce the learning rate by some factor every few epochs. \n",
    "    * Other approaches also avalable, like exponential decay, 1/t decay, etc.\n",
    "- Second-order update method:\n",
    "    * i.e., Newton's method, not common\n",
    "- Per-parameter adaptive learning rate methods: \n",
    "    * For example: Adagrad, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Gradient Descent: \n",
    "    - Global optimal for convex function\n",
    "    - High use of memory; slow\n",
    "    \n",
    "    \n",
    "- Stochatic Gradient Descent\n",
    "    - High speed\n",
    "    - More number of iterations\n",
    "    - For non-convex funciton, may reach better optimal\n",
    "    \n",
    "    \n",
    "- Mini-batch Gradient Descent\n",
    "    - Balance between batch and stochatic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/stochastic-vs-batch-gradient-descent.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent w/o Momentum Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat all elements of $dX$ as a whole in terms of learning rate\n",
    "\n",
    "- If gradient direction not changed, increase update, faster convergence\n",
    "- If gradient direction changed, reduce update, reduce oscillation\n",
    "- Keep most of accumulated direction, and slightly adjust based on new direction\n",
    "- <font color=\"blue\">hard to determine learning rate</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def VanillaUpdate(x, dx, learning_rate):\n",
    "    x += -learning_rate * dx\n",
    "    return x\n",
    "\n",
    "def MomentumUpdate(x, dx, v, learning_rate, mu):\n",
    "    v = mu * v - learning_rate * dx # integrate velocity, mu's typical value is about 0.9\n",
    "    x += v # integrate position     \n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat each element of $dX$ adaptively in terms of learning rate\n",
    "\n",
    "1. Those dx receiving infrequent updates should have higher learning rate. vice versa. - <font color=\"blue\">AdaGrad</font>\n",
    "2. We don't want: the gradients accumulate (too aggressive), and the learning rate monotically decrease. Instead, we want: modulates the learning rate of each weight based on the magnitudes of its gradient only within a recent time window (i.e., less weights for past $dX$) - <font color=\"blue\">RMSprop</font>\n",
    "3. Still want to use \"momentum-like\" update to get a smooth gradient - <font color=\"blue\">Adam</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. AdaGrad\n",
    "def AdaGrad(x, dx, learning_rate, cache, eps):\n",
    "    cache += dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps) # (usually set somewhere in range from 1e-4 to 1e-8)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2. RMSprop\n",
    "def RMSprop(x, dx, learning_rate, cache, eps, decay_rate): #Here, decay_rate typical values are [0.9, 0.99, 0.999]\n",
    "    cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2+3. Adam\n",
    "def Adam(x, dx, learning_rate, m, v, t, beta1, beta2, eps):\n",
    "    m = beta1*m + (1-beta1)*dx # Smooth gradient\n",
    "    #mt = m / (1-beta1**t) # bias-correction step\n",
    "    v = beta2*v + (1-beta2)*(dx**2) # keep track of past updates\n",
    "    #vt = v / (1-beta2**t) # bias-correction step\n",
    "    x += - learning_rate * m / (np.sqrt(v) + eps) # eps = 1e-8, beta1 = 0.9, beta2 = 0.999   \n",
    "    return x, m, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/Optimizers.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Second Order Optimization**\n",
    "- No Hyperparameter and learning rates\n",
    "- N^2 elements, O(N^3) for taking inverting\n",
    "- Methods:\n",
    "    * Quasi-Newton methods(BGFS): O(N^3) -> O(N^2)\n",
    "    * L-BFGS: Does not form/store the full inverse Hessian.\n",
    "    \n",
    "## **Hardware**\n",
    "- CPU: less cores, faster per core, better at sequential tasks\n",
    "- GPU: more cores, slower per core, better at parallel tasks (NVIDIA, CUDA, cuDNN)\n",
    "- TPU: just for DL (Tensor Processing Unit)\n",
    "    * Split *One* graph over *Multiple* machines\n",
    "\n",
    "## **Software**\n",
    "- Caffe (FB)\n",
    "- PyTorch (FB)\n",
    "- TF (Google)\n",
    "- CNTK (MS)\n",
    "- Dynamic (e.g., Eager Execution) vs. Static (e.g., TF Lower-level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Overview\n",
    "\n",
    "- Characteristics\n",
    "    - Spatial data like image, video, text, etc.\n",
    "    - Exploit the local structure of data\n",
    "\n",
    "\n",
    "- Application\n",
    "    - Object detection and localization; for example, pedestrian detection for AV, Facebook friends in photo, etc.\n",
    "    \n",
    "    - Image Segmentation; obtain boundary of each object in the image \n",
    "    \n",
    "**ConvNets**\n",
    "- Difference with regular NN:\n",
    "    * Main difference: each neuron is layer 2 is only connected to a few neurons in layer 1\n",
    "    * Data arranged in 3 dimensions: height, width, and depth\n",
    "    \n",
    "    \n",
    "- Convolutional Layer:\n",
    "    * Filter/Kernel (with full depth, but local connectivity across 2d), 3\\*3 --> 5\\*5\n",
    "    * The depth of layer 2 == The number of filters in Layer 1\n",
    "    * `Stride`: usually 1, leaving downsampling to pooling layer. Can use 2 to compromise 1st layer because of computational constraints\n",
    "    * `Padding`: use same to avoid missing information along the border\n",
    "    * *Parameter Sharing*: Same weight for filter/kernel at same depth slice in layer 2 (Alternative: local)\n",
    "    <img src=\"https://blog.liang2.tw/2015Talk-DeepLearn-CNN/pics/external/cs231n_note_conv_filter.png\" width=\"500\">\n",
    "\n",
    "- Pooling\n",
    "    * Most common: 2-2 Max Pooling\n",
    "\n",
    "\n",
    "- Receptive Field\n",
    "    - For a certain pooling layer, we can find the neurons that contributes most to the activation.\n",
    "<img src=\"https://www.researchgate.net/publication/316950618/figure/fig4/AS:495826810007552@1495225731123/The-receptive-field-of-each-convolution-layer-with-a-3-3-kernel-The-green-area-marks.png\" width=\"300\">\n",
    "    \n",
    "- Fully-Connected Layer\n",
    "\n",
    "\n",
    "- Common architecture:\n",
    "    * INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]\\* -> [FC -> RELU]\\*2 -> FC\n",
    "    * Prefer a stack of small filter CONV to one large receptive field CONV layer\n",
    "    * Stacking three convolutional layers with filters of 3 × 3 and pooling is similar to a single convolutional layer with a 7 × 7 filter.\n",
    "    \n",
    "    \n",
    "- Challenge: Computational resources\n",
    "<img src=\"https://i.stack.imgur.com/AuqKy.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- Different features are detected at each layer. For example, AlexNet:\n",
    "    - More layers can be beneficial for learning complicated features\n",
    "<img src=\"https://banner2.kisspng.com/20180607/vcr/kisspng-deep-learning-convolutional-neural-network-alexnet-deep-learning-5b19637f4372c8.8890220615283905272763.jpg\" width=\"600\">\n",
    "\n",
    "\n",
    "**Transfer Learning**\n",
    "- Apply trained model without last FC layer and use it as feature extracter\n",
    "- Continue to fine tune the model using smaller learning rate\n",
    "- Can use different image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Different architectures\n",
    "- LeNet-5 (CONV-Subsampling-CONV-Subsampling-FC-FC)\n",
    "    - Note: sigmoid activation and no Pooling/Dropout layer\n",
    "    \n",
    "    \n",
    "<img src=\"https://i.stack.imgur.com/tLKYz.png\" width=\"800\">\n",
    "\n",
    "- AlexNet 8 layers (CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8)\n",
    "    - Note: ReLU activation and Pooling/Dropout layer\n",
    "    - Local Response Normalization\n",
    "    \n",
    "    \n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_1.jpg\" width=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ZFNet (similar with AlexNet)\n",
    "    * Smaller kernel, more filters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGGNet (*smaller* filter and *deeper* networks)\n",
    "    * 16-19 layers in VGG16Net\n",
    "    * Three 3 \\* 3 kernel with stride == One 7 \\* 7 kernel; Same *effective receptive field*\n",
    "    * ReLU activation function\n",
    "    * Optimizer: Adam\n",
    "    * Weight initialization: He\n",
    "   \n",
    "    * **deeper network and more non-linearity (more representation power);**\n",
    "    * **less parameters ( 3 \\* 3 \\* 3 vs. 7 \\* 7)**\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/CascadingConvolutions.png\" width=\"500\">\n",
    "<img src=\"http://josephpcohen.com/w/wp-content/uploads/Screen-Shot-2016-01-14-at-11.25.15-AM.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- GoogLeNet\n",
    "    * Introduced *inception* Module (Parallel filter operations with multiple kernel size)\n",
    "    * Problem: Output size too big after filter concatenation\n",
    "    * The purpose of 1 \\* 1 convolutonal layer: \n",
    "        - Pooling layer keeps the same depth as input\n",
    "        - 1 \\* 1 layer keeps the **same dimension** of input, and **reduces depth** (for example: 64 \\* 56 \\* 56 after 32 1 \\* 1 con --> 32 \\* 56 \\* 56)\n",
    "        - Reduce total number of operations\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Bo_Zhao48/publication/312515254/figure/fig3/AS:489373281067012@1493687090916/nception-module-of-GoogLeNet-This-figure-is-from-the-original-paper-10.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- ResNet\n",
    "    * Use network layers to fit a *Residual mapping* instead of directly fitting a desired underlying mapping\n",
    "    * Residual blocks are stacked\n",
    "    * More effective training by enabling gradients to be back-propagated without vanishing\n",
    "    * Similar to GoogLeNet, can use *bottelneck* layer (1 \\* 1 conv layer) for downsampling and efficiency ++\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Antonio_Theophilo/publication/321347448/figure/fig2/AS:565869411815424@1511925189281/Bottleneck-Blocks-for-ResNet-50-left-identity-shortcut-right-projection-shortcut.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/kBFIf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of filters for each layer \n",
    "\n",
    "|CNN|Convolutional layer filter count progression|\n",
    "|:-:|:-:|\n",
    "|LeNet\t|20, 50\n",
    "|AlexNet\t|96, 256, 384, 384, 256\n",
    "|VGGNet|\t64, 128, 256, 512, 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Augmentation\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*dJNlEc7yf93K4pjRJL55PA.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Features:\n",
    "- Earlier layers: generic visual feature\n",
    "- Later layers: dataset specific features\n",
    "\n",
    "\n",
    "Strategy:\n",
    "- Replace last classifier layer\n",
    "- Remove classifier (softmax) and FC layer. Use network as a feature extractor\n",
    "- Use smaller learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Basic RNN and Gradient Vanishing\n",
    "\n",
    "<img src=\"http://corochann.com/wp-content/uploads/2017/05/rnn1_graph-800x460.png\" width=500>\n",
    "\n",
    "- What is the problem with RNN\n",
    "    - <font color = \"red\">Gradient Vanishing</font> and Exploding with Vanilla RNN\n",
    "    - Computing gradient of $h_0$ involved many multiplication of ***W*** and ***tanh*** activation (one small gradient would carry over)\n",
    "    - Brief proof:\n",
    "    \n",
    "    $$\\frac{ \\partial E_t} { \\partial w}= \\sum_{k=1}^{t}  \\frac{ \\partial E_t} { \\partial y_t}\\frac{ \\partial y_t} { \\partial h_t}\\frac{ \\partial h_t} { \\partial h_k}\\frac{ \\partial h_k} { \\partial w}$$          \n",
    "    $$\\frac{ \\partial h_t} { \\partial h_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}}$$\n",
    "    \n",
    "    where $$ h_j = tanh(W_{hx}X_j + W_{hh}h_{j-1} + b)$$\n",
    "    then $$\\frac{ \\partial h_t} { \\partial h_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}} =  \\prod_{j= k + 1}^{t} tanh'(\\cdot)W_{hh}$$\n",
    "    \n",
    "    note that for tanh function, $tanh'(\\cdot)<=1$: <img src=\"./figure/tanh.png\" width=\"300\">\n",
    "\n",
    "    - Depending on $W_{hh}$, we may have gradient vanish or explode\n",
    "    - We cannot figure out the dependency between long time interval's data\n",
    "\n",
    "<img src=\"./figure/rnn2.png\" width=\"500\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to fix vanishing gradients?\n",
    "    - Partial fix for gradient exploding: if ||g|| > threshold, shrink value of g\n",
    "    - Initialize W to be identity\n",
    "    - Use ReLU as activation function f\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of RNN\n",
    "- Language modelling (see nlp repo for details)\n",
    "- seq2seq models for machine translations (see nlp repo for details)\n",
    "- seq2seq with attention (see nlp repo for details)\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0706.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "- Main Idea of LSTM\n",
    "    * **Forget Gate** (\\*): how much old memory we want to keep; element-wise multiplication with old memory $ C_{t-1} $. The Parameters are learned as $ W_f $. I.e., $ \\sigma(W_f([h_{t-1}, X_t]) + b_f = f_t $. If you want all old memory, then $ f_t $ equals 1. After getting $ f_t $, multiply it with $ C_{t-1} $<br/><br/>\n",
    "   \n",
    "    * **New Memory Gate**(\\+)\n",
    "        - How to merge new memory with old memory; piece-wise summation, decides how to combine *new* memory with *old* memory. The weighing parameters are learned as $ W_i $. I.e., $ \\sigma(W_i([h_{t-1}, X_t]) + b_i = i_t $. \n",
    "    \n",
    "        - What is the new memory itself: $ tanh(W_C([h_{t-1}, X_t]) + b_C = \\tilde{C_t} $\n",
    "    \n",
    "        - What is the combined memory: $ C_{t-1} * f_t + \\tilde{C_t} * i_t = C_t$ <br/><br/>\n",
    "        \n",
    "    * **Output gate**: how much of the new memory we want to output or store? learned solely through combined memory. $ \\sigma(W_o([h_{t-1}, X_t]) + b_o = o_t $. Then the final output $ h_t $ would be $ o_t * tanh(C_t) = h_t $\n",
    "    \n",
    "    \n",
    "    \n",
    "- Why LSTM prevents gradient vanishing?\n",
    "    - *Linear* Connection between $C_t$ and $C_{t-1}$ rather than multiplying\n",
    "    - Forget gate controls and keeps long-distance dependency\n",
    "    - Allows error to flow at different strength based on inputs\n",
    "    - During initialization: Initialize forget gate bias to one: default to remembering\n",
    "    - Compared with Vanilla RNN, $\\frac{ \\partial C_t} { \\partial C_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial C_j} { \\partial C_{j-1} }= \\prod_{j= k + 1}^{t} \\sigma'(\\cdot) \\approx 0|1$, where $\\cdot$ is input to forget gate.\n",
    "    - Key difference with RNN: we no longer have the term: $\\prod_{j= k + 1}^{t} W_{hh}$\n",
    "    - See proof: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/bi-lstm.png\" width=\"300\">\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/char_representation.png\" width=\"300\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other variation: Gated Recurrent Unit (GRU)\n",
    "- **Update Gate**: How to combine old and new state: $ \\sigma(W_z([h_{t-1}, X_t])  = z_t $\n",
    "- **Reset Gate**: How much to keep old state: $ \\sigma(W_r([h_{t-1}, X_t])= r_t $\n",
    "- **New State**: $ tanh(WX_t + r_t * U h_{t-1}) =\\tilde{h_t}$ \n",
    "- **Combine States**: $z_t* h_{t-1} + (1-z_t) * \\tilde{h_t} $\n",
    "- If r=0, ignore/drop previous state for generating new state\n",
    "- if z=1, carry information from past through many steps (long-term dependency)\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/ce3f6d276f10fef2c145daf0fe19c006c92ce019/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d7661722d4752552e706e67\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0705.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "1. From image: [CONV-POOL] \\* n --> FC Layer --> (num_example, 4096) written as **v**\n",
    "2. Hiddern layer: $h = tanh(W_{xh} * X + W_{hh} * h + W_{ih} * \\bf{v} )$\n",
    "3. Output layer: $y = W_{hy} * h$\n",
    "4. Get input $ X_{t+1}$ by sampling $\\bf{y} $\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0108.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Workflow\n",
    "- Preprocess Data (zero-centered,i.e.,Substract Mean)\n",
    "- Identify architecture\n",
    "- Ensure that we can overfit a small traing set to acc = 100%\n",
    "- Loss not decreasing: too low leaning rate\n",
    "- Loss goes to NaN: too high learning rate (e.g., log(0) = NaN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guideline:\n",
    "- Observe training loss and validation loss curve\n",
    "- Use a subset of data for hyperparameters tuning\n",
    "- Add shuffling for input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/11/cnn-keras-curves-without-aug.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting:\n",
    "    - Increase Dropout ratio\n",
    "    - Decrease layers\n",
    "    - Increase regularization (L1, L2)\n",
    "    - Early stopping\n",
    "    - Observe distribution of weights (any extreme values)\n",
    "    \n",
    "    \n",
    "- Underfitting:\n",
    "    - Increase layers\n",
    "    - Decrease regularization\n",
    "    - Decrease learning rate\n",
    "    \n",
    "\n",
    "- Gradient vanishing\n",
    "    - Batch Normalization\n",
    "    - Less layers\n",
    "    - Appropriate activation function (Relu instead of Sigmoid)\n",
    "    \n",
    "    \n",
    "- Local minima\n",
    "    - Increase learning rate\n",
    "    - Appropriate optimizer\n",
    "    - Smaller batch size\n",
    "\n",
    "\n",
    "- Memory concerns\n",
    "    - What determines memory requirment? \n",
    "    - Number of parameters\n",
    "    - Type of updater (SGD/Momentum/Adam)\n",
    "    - Mini-batch size\n",
    "    - Size of single sample\n",
    "    - Number of activations\n",
    "    \n",
    "    \n",
    "- Learning Rate\n",
    "    - Observe the relative change of parameters $\\frac{\\Delta w}{|w|}$ to be around $10^{-3}$.\n",
    "    - Increase/Decrease the learing rate accordingly\n",
    "    <img src=\"https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0602.png\" width=\"300\">\n",
    "    - Strategy: start with a relatively large value, and observe if the training process diverges (i.e., training loss increases). And decrease the rate until convergence. Start with values like 0.001.\n",
    "    \n",
    "    \n",
    "- Mini-batch size\n",
    "    - Biggger Mini-batch gives smoother gradient update but longer to run\n",
    "    - In practice, 32 to 256 is common for CPU training, and 32 to 1,024 is common for GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to read: https://pcc.cs.byu.edu/2017/10/02/practical-advice-for-building-deep-neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware\n",
    "- CPU\n",
    "- GPU (for parallel)\n",
    "- TPU (for matrix)\n",
    "\n",
    "## Parelleling\n",
    "\n",
    "\n",
    "- Data parallel training splits large datasets across multiple computing nodes\n",
    "    - Each worker node has a complete copy of the model being trained.\n",
    "    - One single as supervisor server.\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0908.png\" width=\"200\">\n",
    "\n",
    "\n",
    "- Model parallel training splits large models across multiple nodes. \n",
    "    - Motivation: memory restriction on a single CPU\n",
    "    - Network itself is stored on multiple CPUs\n",
    "    - Not common..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "632px",
    "left": "0px",
    "right": "1357.17px",
    "top": "106.992px",
    "width": "221.847px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
