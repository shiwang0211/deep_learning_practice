{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some key concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Background**\n",
    "- feature engineering and SVM\n",
    "- conputational speed and GPU\n",
    "- dataset size and internet, orders of magnitude more data\n",
    "- Framework such as TF, Keras\n",
    "\n",
    "**NN as universal approximators**\n",
    "- More neurons --> more complicated functions\n",
    "- Regularization to prevent overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Activation Function**\n",
    "- Sigmoid\n",
    "    * Saturated at 0/1 and kills gradients (derivative -> 0)\n",
    "    * Output not zero-centred; for next layer: f = wx + b, x>0, df/dw same sign for all w; zig-zag update trajectory\n",
    "    \n",
    "    \n",
    "- Tanh\n",
    "    * Still kills gradients\n",
    "    * But: zero-cented\n",
    "    \n",
    "    \n",
    "- ReLU\n",
    "    * Non-saturated, linearity --> Accelerate convergence\n",
    "    * Cheap computation\n",
    "    * But: Can die; never activate\n",
    "    * Extension: Leaky ReLU, maxout\n",
    "\n",
    "\n",
    "- Leaky ReLU\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1200/1*ZafDv3VUm60Eh10OeJu1vw.png\" width=\"500\">\n",
    "\n",
    "\n",
    "## **Regularization**\n",
    "- L1/L2/ElasticNet\n",
    "- Max Norm constraint\n",
    "- Drop Out layer\n",
    "\n",
    "## **Hyperparameter Optimization**\n",
    "- Single validation set > cross validation in practice\n",
    "- Random search instead of grid search within a range\n",
    "- Metric selection: accuracy, RMSE, etc.\n",
    "- Ideally: Training + Validation + Test, where validation set is to *\"learn\"* hyperparameters. \n",
    "    - Think of hyperparam tuning itself a learning algorithm and validation set becomes \"training set\" in the new problem.\n",
    "\n",
    "\n",
    "- Important hyperparams:\n",
    "    - Network structure\n",
    "    - Batch size\n",
    "    - Learing rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Weights Initialization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **All zero**: \n",
    "    - Wrong: neuron outputs and gradients would be same; same update\n",
    "    - We don't want the symmetric structure of weights\n",
    "    \n",
    "    \n",
    "- **Number to small**: \n",
    "    - small gradients for hidden layer inputs ($\\frac{\\partial L}{\\partial h} = W$)\n",
    "    - gradient diminishing when flowing backwafrd\n",
    "    \n",
    "    \n",
    "- **Preferred: All neuron with a good output distribution to feed next layer**:\n",
    "    - Benefit: effective back-propagation for weights\n",
    "    - w = np.random.randn(n) / sqrt(n), where n is number of inputs. In other words, the standard error $std(w) = \\sqrt\\frac {1}{n}$\n",
    "    - It can be proved that Var(S) = Var(WX) = Var(X)\n",
    "    - It helps get a wide range of output value for each hidden layer\n",
    "    - For activation functions ReLU, see below: $std(w) = \\sqrt\\frac {2}{n}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **An example**\n",
    "    - An example shows how weight initialization helps keep the same distribution of each layer's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def ReLU(x):\n",
    "    return x * (x > 0)\n",
    "\n",
    "def get_activation(w, activation):\n",
    "    x = np.random.randn(10000, 100) \n",
    "    node_num = 100\n",
    "    hidden_layer_size = 5\n",
    "    activations = {}\n",
    "    for i in range(hidden_layer_size):\n",
    "        if i != 0:\n",
    "            x = activations[i-1]\n",
    "        z = np.dot(x, w)\n",
    "        if activation == \"sigmoid\":\n",
    "            a = sigmoid(z)\n",
    "        elif activation == \"ReLU\":\n",
    "            a = ReLU(z)\n",
    "        activations[i] = a\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w = np.random.randn(node_num, node_num) * 1 / np.sqrt(node_num)\n",
    "activations = get_activation(w, 'sigmoid')\n",
    "for i, a in activations.items():\n",
    "    plt.subplot(len(activations),2 , 2 * i + 1)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0,1))\n",
    "    \n",
    "w = np.random.randn(node_num, node_num) * 2 / np.sqrt(node_num)\n",
    "activations = get_activation(w, 'ReLU')    \n",
    "for i, a in activations.items():\n",
    "    plt.subplot(len(activations),2 , 2 + 2 * i)\n",
    "    plt.title(str(i+1) + \"-layer\")\n",
    "    plt.hist(a.flatten(), 30, range=(0.01,1))   \n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dropout**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Reduce interaction/co-adapt between units -> better generalization\n",
    "- Similar to subsampling/bagging on network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./figure/dropout.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Batch Normalization**\n",
    "###  Internal Covariate Shift\n",
    "- Different layers have different distributions of input data\n",
    "    \n",
    "    - 1. Gradient Vanishing: activation function input value within nonlinear regime for sigmoid function <img src=\"https://image.slidesharecdn.com/dlmmdcud1l06optimization-170427160940/95/optimizing-deep-networks-d1l6-insightdcu-machine-learning-workshop-2017-8-638.jpg?cb=1493309658\" width=\"500\">\n",
    "\n",
    "    - 2. Slow learning: different scale makes it harder for faster convergence using SGD\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_5/FeatureScaling.jpg\" width=\"500\">\n",
    "\n",
    "\n",
    "### Algorithm\n",
    "1. **Shift and scale the output values to $N(0,1)$**\n",
    "    - <img src=\"https://guillaumebrg.files.wordpress.com/2016/02/bn.png?w=656\" width=\"500\">\n",
    "\n",
    "    - Improve gradient flow (point1)\n",
    "    - Allow higher learning rates (point2)\n",
    "    - Reduce dependence on initialization (output distribution no long depends on weights $W$)\n",
    "    - *Note*: At test time, the mean from training should be used instead of calculated from testing batch\n",
    "\n",
    "\n",
    "2. **Learn $\\gamma$ and $\\beta$ to retrieve representation power**\n",
    "    - The distribution of the feature may be located at two sides of the non-linear regions for sigmoid funciton, and forcing it to be standardized will lose the distribution.\n",
    "    \n",
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" width=\"400\">\n",
    "\n",
    "\n",
    "<img src=\"https://qph.fs.quoracdn.net/main-qimg-36a6ee9c550f479fc681eab380510baf\" width=\"500\">\n",
    "\n",
    "3. **How to predict**\n",
    "    - Use the unbiased estimation of mean and variance from all train batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Param Update and Learning Rate**\n",
    "- Step decay for learning rate: \n",
    "    * Reduce the learning rate by some factor every few epochs. \n",
    "    * Other approaches also avalable, like exponential decay, 1/t decay, etc.\n",
    "- Second-order update method:\n",
    "    * i.e., Newton's method, not common\n",
    "- Per-parameter adaptive learning rate methods: \n",
    "    * For example: Adagrad, Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Batch Gradient Descent: \n",
    "    - Global optimal for convex function\n",
    "    - High use of memory; slow\n",
    "    \n",
    "    \n",
    "- Stochatic Gradient Descent\n",
    "    - High speed\n",
    "    - More number of iterations\n",
    "    - For non-convex funciton, may reach better optimal\n",
    "    \n",
    "    \n",
    "- Mini-batch Gradient Descent\n",
    "    - Balance between batch and stochatic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.bogotobogo.com/python/scikit-learn/images/Batch-vs-Stochastic-Gradient-Descent/stochastic-vs-batch-gradient-descent.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent w/o Momentum Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat all elements of $dX$ as a whole in terms of learning rate\n",
    "\n",
    "- If gradient direction not changed, increase update, faster convergence\n",
    "- If gradient direction changed, reduce update, reduce oscillation\n",
    "- Keep most of accumulated direction, and slightly adjust based on new direction\n",
    "- <font color=\"blue\">hard to determine learning rate</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def VanillaUpdate(x, dx, learning_rate):\n",
    "    x += -learning_rate * dx\n",
    "    return x\n",
    "\n",
    "def MomentumUpdate(x, dx, v, learning_rate, mu):\n",
    "    v = mu * v - learning_rate * dx # integrate velocity, mu's typical value is about 0.9\n",
    "    x += v # integrate position     \n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://cs231n.github.io/assets/nn3/nesterov.jpeg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treat each element of $dX$ adaptively in terms of learning rate\n",
    "\n",
    "1. Those dx receiving infrequent updates should have higher learning rate. vice versa. - <font color=\"blue\">AdaGrad</font>\n",
    "2. We don't want: the gradients accumulate (too aggressive), and the learning rate monotically decrease. Instead, we want: modulates the learning rate of each weight based on the magnitudes of its gradient only within a recent time window (i.e., less weights for past $dX$) - <font color=\"blue\">RMSprop</font>\n",
    "3. Still want to use \"momentum-like\" update to get a smooth gradient - <font color=\"blue\">Adam</font>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 1. AdaGrad\n",
    "def AdaGrad(x, dx, learning_rate, cache, eps):\n",
    "    cache += dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps) # (usually set somewhere in range from 1e-4 to 1e-8)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2. RMSprop\n",
    "def RMSprop(x, dx, learning_rate, cache, eps, decay_rate): #Here, decay_rate typical values are [0.9, 0.99, 0.999]\n",
    "    cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2+3. Adam\n",
    "def Adam(x, dx, learning_rate, m, v, t, beta1, beta2, eps):\n",
    "    m = beta1*m + (1-beta1)*dx # Smooth gradient\n",
    "    #mt = m / (1-beta1**t) # bias-correction step\n",
    "    v = beta2*v + (1-beta2)*(dx**2) # keep track of past updates\n",
    "    #vt = v / (1-beta2**t) # bias-correction step\n",
    "    x += - learning_rate * m / (np.sqrt(v) + eps) # eps = 1e-8, beta1 = 0.9, beta2 = 0.999   \n",
    "    return x, m, v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./fig/Optimizers.png\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **General Workflow**\n",
    "- Preprocess Data (zero-centered,i.e.,Sub-stractmean)\n",
    "- Identify architecture\n",
    "- Ensure that we can overfit a small traing set to acc = 100%\n",
    "- Loss not decreasing: too low leaning rate\n",
    "- Loss goes to NaN: too high learning rate  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Second Order Optimization**\n",
    "- No Hyperparameter and learning rates\n",
    "- N^2 elements, O(N^3) for taking inverting\n",
    "- Methods:\n",
    "    * Quasi-Newton methods(BGFS): O(N^3) -> O(N^2)\n",
    "    * L-BFGS: Does not form/store the full inverse Hessian.\n",
    "    \n",
    "## **Hardware**\n",
    "- CPU: less cores, faster per core, better at sequential tasks\n",
    "- GPU: more cores, slower per core, better at parallel tasks (NVIDIA, CUDA, cuDNN)\n",
    "- TPU: just for DL (Tensor Processing Unit)\n",
    "    * Split *One* graph over *Multiple* machines\n",
    "\n",
    "## **Software**\n",
    "- Caffe (FB)\n",
    "- PyTorch (FB)\n",
    "- TF (Google)\n",
    "- CNTK (MS)\n",
    "- Dynamic (e.g., Eager Execution) vs. Static (e.g., TF Lower-level API)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN\n",
    "## Overview\n",
    "\n",
    "- Characteristics\n",
    "    - Spatial data like image, video, text, etc.\n",
    "    - Exploit the local structure of data\n",
    "\n",
    "\n",
    "- Application\n",
    "    - Object detection and localization; for example, pedestrian detection for AV, Facebook friends in photo, etc.\n",
    "    \n",
    "    - Image Segmentation; obtain boundary of each object in the image \n",
    "    \n",
    "**ConvNets**\n",
    "- Difference with regular NN:\n",
    "    * Main difference: each neuron is layer 2 is only connected to a few neurons in layer 1\n",
    "    * Data arranged in 3 dimensions: height, width, and depth\n",
    "    \n",
    "    \n",
    "- Convolutional Layer:\n",
    "    * Filter/Kernel (with full depth, but local connectivity across 2d), 3\\*3 --> 5\\*5\n",
    "    * The depth of layer 2 == The number of filters in Layer 1\n",
    "    * `Stride`: usually 1, leaving downsampling to pooling layer. Can use 2 to compromise 1st layer because of computational constraints\n",
    "    * `Padding`: use same to avoid missing information along the border\n",
    "    * *Parameter Sharing*: Same weight for filter/kernel at same depth slice in layer 2 (Alternative: local)\n",
    "    \n",
    "    \n",
    "- Pooling\n",
    "    * Most commonly: 2-2 Max Pooling\n",
    "    \n",
    "    \n",
    "- Fully-Connected Layer\n",
    "\n",
    "\n",
    "- Common architecture:\n",
    "    * INPUT -> [CONV -> RELU -> CONV -> RELU -> POOL]\\* -> [FC -> RELU]\\*2 -> FC\n",
    "    * Prefer a stack of small filter CONV to one large receptive field CONV layer\n",
    "    * Stacking three convolutional layers with filters of 3 × 3 and pooling is similar to a single convolutional layer with a 7 × 7 filter.\n",
    "    \n",
    "    \n",
    "- Challenge: Computational resources\n",
    "<img src=\"https://i.stack.imgur.com/AuqKy.png\" width=\"500\">\n",
    "\n",
    "\n",
    "- Different features are detected at each layer. For example, AlexNet:\n",
    "    - More layers can be beneficial for learning complicated features\n",
    "<img src=\"https://banner2.kisspng.com/20180607/vcr/kisspng-deep-learning-convolutional-neural-network-alexnet-deep-learning-5b19637f4372c8.8890220615283905272763.jpg\" width=\"600\">\n",
    "**Transfer Learning**\n",
    "- Apply trained model without last FC layer and use it as feature extracter\n",
    "- Continue to fine tune the model using smaller learning rate\n",
    "- Can use different image size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    \n",
    "## Different architectures\n",
    "- LeNet-5 (CONV-Subsampling-CONV-Subsampling-FC-FC)\n",
    "    - Note: sigmoid activation and no Pooling/Dropout layer\n",
    "<img src=\"https://i.stack.imgur.com/tLKYz.png\" width=\"800\">\n",
    "\n",
    "- AlexNet 8 layers (CONV1-MAXPOOL1-NORM1-CONV2-MAXPOOL2-NORM2-CONV3-CONV4-CONV5-MAXPOOL3-FC6-FC7-FC8)\n",
    "    - Note: ReLU activation and Pooling/Dropout layer\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0106.png\" width=\"400\">\n",
    "\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_7/AlexNet_1.jpg\" width=\"300\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ZFNet (similar with AlexNet)\n",
    "    * Smaller kernel, more filters\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- VGGNet(*smaller* filter and *deeper* networks)\n",
    "    * 16-19 layers in VGG16Net\n",
    "    * Three 3 \\* 3 kernel with stride == One 7 \\* 7 kernel; Same *effective receptive field*\n",
    "    * ReLU activation function\n",
    "    * Optimizer: Adam\n",
    "    * Weight initialization: He\n",
    "   \n",
    "    * **deeper network and more non-linearity (more representation power);**\n",
    "    * **less parameters ( 3 \\* 3 \\* 3 vs. 7 \\* 7)**\n",
    "<img src=\"https://leonardoaraujosantos.gitbooks.io/artificial-inteligence/content/image_folder_3/CascadingConvolutions.png\" width=\"500\">\n",
    "<img src=\"http://josephpcohen.com/w/wp-content/uploads/Screen-Shot-2016-01-14-at-11.25.15-AM.png\" width=\"800\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- GoogLeNet\n",
    "    * Introduced *inception* Module (Parallel filter operations with multiple kernel size)\n",
    "    * Problem: Output size too big after filter concatenation\n",
    "    * The purpose of 1 \\* 1 convolutonal layer: \n",
    "        - Pooling layer keeps the same depth as input\n",
    "        - 1 \\* 1 layer keeps the **same dimension** of input, and **reduces depth** (for example: 64 \\* 56 \\* 56 after 32 1 \\* 1 con --> 32 \\* 56 \\* 56)\n",
    "        - Reduce total number of operations\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Bo_Zhao48/publication/312515254/figure/fig3/AS:489373281067012@1493687090916/nception-module-of-GoogLeNet-This-figure-is-from-the-original-paper-10.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "- ResNet\n",
    "    * Use network layers to fit a *Residual mapping* instead of directly fitting a desired underlying mapping\n",
    "    * Residual blocks are stacked\n",
    "    * More effective training by enabling gradients to be back-propagated without vanishing\n",
    "    * Similar to GoogLeNet, can use *bottelneck* layer (1 \\* 1 conv layer) for downsampling and efficiency ++\n",
    "    \n",
    "<img src=\"https://www.researchgate.net/profile/Antonio_Theophilo/publication/321347448/figure/fig2/AS:565869411815424@1511925189281/Bottleneck-Blocks-for-ResNet-50-left-identity-shortcut-right-projection-shortcut.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://i.stack.imgur.com/kBFIf.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of filters for each layer \n",
    "\n",
    "|CNN|Convolutional layer filter count progression|\n",
    "|:-:|:-:|\n",
    "|LeNet\t|20, 50\n",
    "|AlexNet\t|96, 256, 384, 384, 256\n",
    "|VGGNet|\t64, 128, 256, 512, 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data Augmentation\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/1*dJNlEc7yf93K4pjRJL55PA.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning\n",
    "\n",
    "Features:\n",
    "- Earlier layers: generic visual feature\n",
    "- Later layers: dataset specific features\n",
    "\n",
    "\n",
    "Strategy:\n",
    "- Replace last classifier layer\n",
    "- Remove classifier (softmax) and FC layer. Use network as a feature extractor\n",
    "- Use smaller learning rates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "## Basic RNN and Gradient Vanishing\n",
    "\n",
    "<img src=\"http://corochann.com/wp-content/uploads/2017/05/rnn1_graph-800x460.png\" width=500>\n",
    "\n",
    "- What is the problem with RNN\n",
    "    - <font color = \"red\">Gradient Vanishing</font> and Exploding with Vanilla RNN\n",
    "    - Computing gradient of $h_0$ involved many multiplication of ***W*** and ***tanh*** activation (one small gradient would carry over)\n",
    "    - Brief proof:\n",
    "    \n",
    "    $$\\frac{ \\partial E_t} { \\partial w}= \\sum_{k=1}^{t}  \\frac{ \\partial E_t} { \\partial y_t}\\frac{ \\partial y_t} { \\partial h_t}\\frac{ \\partial h_t} { \\partial h_k}\\frac{ \\partial h_k} { \\partial w}$$          \n",
    "    $$\\frac{ \\partial h_t} { \\partial h_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}}$$\n",
    "    \n",
    "    where $$ h_j = tanh(W_{hx}X_j + W_{hh}h_{j-1} + b)$$\n",
    "    then $$\\frac{ \\partial h_t} { \\partial h_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1}} =  \\prod_{j= k + 1}^{t} tanh'(\\cdot)W_{hh}$$\n",
    "    \n",
    "    note that for tanh function, $tanh'(\\cdot)<=1$: <img src=\"./figure/tanh.png\" width=\"300\">\n",
    "\n",
    "    - Depending on $W_{hh}$, we may have gradient vanish or explode\n",
    "    - We cannot figure out the dependency between long time interval's data\n",
    "\n",
    "<img src=\"./figure/rnn2.png\" width=\"500\">  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to fix vanishing gradients?\n",
    "    - Partial fix for gradient exploding: if ||g|| > threshold, shrink value of g\n",
    "    - Initialize W to be identity\n",
    "    - Use ReLU as activation function f\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applications of RNN\n",
    "- Language modelling (see nlp repo for details)\n",
    "- seq2seq models for machine translations (see nlp repo for details)\n",
    "- seq2seq with attention (see nlp repo for details)\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0706.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "- Main Idea of LSTM\n",
    "    * **Forget Gate** (\\*): how much old memory we want to keep; element-wise multiplication with old memory $ C_{t-1} $. The Parameters are learned as $ W_f $. I.e., $ \\sigma(W_f([h_{t-1}, X_t]) + b_f = f_t $. If you want all old memory, then $ f_t $ equals 1. After getting $ f_t $, multiply it with $ C_{t-1} $<br/><br/>\n",
    "   \n",
    "    * **New Memory Gate**(\\+)\n",
    "        - How to merge new memory with old memory; piece-wise summation, decides how to combine *new* memory with *old* memory. The weighing parameters are learned as $ W_i $. I.e., $ \\sigma(W_i([h_{t-1}, X_t]) + b_i = i_t $. \n",
    "    \n",
    "        - What is the new memory itself: $ tanh(W_C([h_{t-1}, X_t]) + b_C = \\tilde{C_t} $\n",
    "    \n",
    "        - What is the combined memory: $ C_{t-1} * f_t + \\tilde{C_t} * i_t = C_t$ <br/><br/>\n",
    "        \n",
    "    * **Output gate**: how much of the new memory we want to output or store? learned solely through combined memory. $ \\sigma(W_o([h_{t-1}, X_t]) + b_o = o_t $. Then the final output $ h_t $ would be $ o_t * tanh(C_t) = h_t $\n",
    "    \n",
    "    \n",
    "    \n",
    "- Why LSTM prevents gradient vanishing?\n",
    "    - *Linear* Connection between $C_t$ and $C_{t-1}$ rather than multiplying\n",
    "    - Forget gate controls and keeps long-distance dependency\n",
    "    - Allows error to flow at different strength based on inputs\n",
    "    - During initialization: Initialize forget gate bias to one: default to remembering\n",
    "    - Compared with Vanilla RNN, $\\frac{ \\partial h_t} { \\partial h_k}= \\prod_{j= k + 1}^{t} \\frac{ \\partial h_j} { \\partial h_{j-1} }= \\prod_{j= k + 1}^{t} tanh' \\sigma(\\cdot) \\approx 0|1$\n",
    "    - See proof: https://weberna.github.io/blog/2017/11/15/LSTM-Vanishing-Gradients.html\n",
    "\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/1600/0*LyfY3Mow9eCYlj7o.\" width=\"600\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional LSTM\n",
    "<img src=\"https://guillaumegenthial.github.io/assets/char_representation.png\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other variation: Gated Recurrent Unit (GRU)\n",
    "- **Update Gate**: How to combine old and new state: $ \\sigma(W_z([h_{t-1}, X_t])  = z_t $\n",
    "- **Reset Gate**: How much to keep old state: $ \\sigma(W_r([h_{t-1}, X_t])= r_t $\n",
    "- **New State**: $ tanh(WX_t + r_t * U h_{t-1}) =\\tilde{h_t}$ \n",
    "- **Combine States**: $z_t* h_{t-1} + (1-z_t) * \\tilde{h_t} $\n",
    "- If r=0, ignore/drop previous state for generating new state\n",
    "- if z=1, carry information from past through many steps (long-term dependency)\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/ce3f6d276f10fef2c145daf0fe19c006c92ce019/687474703a2f2f636f6c61682e6769746875622e696f2f706f7374732f323031352d30382d556e6465727374616e64696e672d4c53544d732f696d672f4c53544d332d7661722d4752552e706e67\" width=\"600\">\n",
    "\n",
    "---\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0705.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Captioning\n",
    "1. From image: [CONV-POOL] \\* n --> FC Layer --> (num_example, 4096) written as **v**\n",
    "2. Hiddern layer: $h = tanh(W_{xh} * X + W_{hh} * h + W_{ih} * \\bf{v} )$\n",
    "3. Output layer: $y = W_{hy} * h$\n",
    "4. Get input $ X_{t+1}$ by sampling $\\bf{y} $\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/yunjey/pytorch-tutorial/master/tutorials/03-advanced/image_captioning/png/model.png\" width=\"500\">\n",
    "\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0108.png\" width=\"500\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General guideline:\n",
    "- Observe training loss and validation loss curve\n",
    "- Use a subset of data for hyperparameters tuning\n",
    "- Add shuffling for input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.learnopencv.com/wp-content/uploads/2017/11/cnn-keras-curves-without-aug.jpg\" width=\"400\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Overfitting:\n",
    "    - Increase Dropout ratio\n",
    "    - Decrease layers\n",
    "    - Increase regularization\n",
    "    - Early stopping\n",
    "    - Observe distribution of weights (any extreme values)\n",
    "    \n",
    "    \n",
    "- Underfitting:\n",
    "    - Increase layers\n",
    "    - Decrease regularization\n",
    "    - Decrease learning rate\n",
    "    \n",
    "\n",
    "- Gradient vanishing\n",
    "    - Batch Normalization\n",
    "    - Less layers\n",
    "    - Appropriate activation function\n",
    "    \n",
    "    \n",
    "- Local minima\n",
    "    - Increase learning rate\n",
    "    - Appropriate optimizer\n",
    "    - Smaller batch size\n",
    "\n",
    "\n",
    "- Memory concerns\n",
    "    - What determines memory requirment? \n",
    "    - Number of parameters\n",
    "    - Type of updater (SGD/Momentum/Adam)\n",
    "    - Mini-batch size\n",
    "    - Size of single sample\n",
    "    - Number of activations\n",
    "    \n",
    "    \n",
    "- Learning Rate\n",
    "    - Observe the relative change of parameters $\\frac{\\Delta w}{|w|}$ to be around $10^{-3}$.\n",
    "    - Increase/Decrease the learing rate accordingly\n",
    "    <img src=\"https://www.safaribooksonline.com/library/view/deep-learning/9781491924570/assets/dpln_0602.png\" width=\"300\">\n",
    "    - Strategy: start with a relatively large value, and observe if the training process diverges (i.e., training loss increases). And decrease the rate until convergence. Start with values like 0.001.\n",
    "    \n",
    "    \n",
    "- Mini-batch size\n",
    "    - Biggger Mini-batch gives smoother gradient update but longer to run\n",
    "    - In practice, 32 to 256 is common for CPU training, and 32 to 1,024 is common for GPU training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware\n",
    "- CPU\n",
    "- GPU (for parallel)\n",
    "- TPU (for matrix)\n",
    "\n",
    "## Parelleling\n",
    "\n",
    "\n",
    "- Data parallel training splits large datasets across multiple computing nodes\n",
    "    - Each worker node has a complete copy of the model being trained.\n",
    "    - One single as supervisor server.\n",
    "<img src=\"https://www.safaribooksonline.com/library/view/tensorflow-for-deep/9781491980446/assets/tfdl_0908.png\" width=\"200\">\n",
    "\n",
    "\n",
    "- Model parallel training splits large models across multiple nodes. \n",
    "    - Motivation: memory restriction on a single CPU\n",
    "    - Network itself is stored on multiple CPUs\n",
    "    - Not common..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "632px",
    "left": "0px",
    "right": "1357.17px",
    "top": "106.992px",
    "width": "342px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
