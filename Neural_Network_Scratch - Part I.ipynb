{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Write Neural Network from Scratch using Numpy**\n",
    "<br/><br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Gates (Add, Multiply)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some Notations**\n",
    "\n",
    "$$\n",
    "WX + b = \\mathbf{S} \\\\\n",
    "\\frac{\\partial{L}}{\\partial{WX}} = d(WX) \\\\\n",
    "\\frac{\\partial{L}}{\\partial{S}} = d({S}) \\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Math: http://cs231n.github.io/optimization-2/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Propagation.py\n",
    "\n",
    "import numpy as np\n",
    "from util.im2col import *\n",
    "from script.Optimization import *\n",
    "class Mul:\n",
    "    def forward(self, W, X):\n",
    "        return np.dot(X, W)\n",
    "    \n",
    "    def backward(self, W, X, dWX):\n",
    "        dW = np.dot( np.transpose(X), dWX )\n",
    "        dX = np.dot( dWX, np.transpose (W))\n",
    "        return dW, dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Add:\n",
    "    def forward(self, WX, b):\n",
    "        return WX + b\n",
    "\n",
    "    def backward(self, WX, b, dS):\n",
    "        dWX = dS * np.ones_like(WX, dtype=np.float64)\n",
    "        db = np.dot(np.ones((1, dS.shape[0]), dtype=np.float64), dS)\n",
    "        return db, dWX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "Activation(S) = Z \\\\\n",
    "Input: \n",
    "\\frac{\\partial{L}}{\\partial{Z}} = d(Z) \\\\\n",
    "Output:\n",
    "\\frac{\\partial{L}}{\\partial{S}} = d(S) \\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class ReLU:\n",
    "    def forward(self, S):\n",
    "        Z = S * (S > 0)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        dS = 1. * (S > 0) * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Tanh:\n",
    "    def forward(self, S):\n",
    "        Z = np.tanh(S)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        Z = self.forward(S)\n",
    "        dS = (1.0 - np.square(Z)) * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Sigmoid:\n",
    "    def forward(self, S):\n",
    "        Z = 1. / (1.0 + np.exp(-S))\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, S, dZ):\n",
    "        Z = self.forward(S)\n",
    "        dS =(1 - Z) * Z * dZ\n",
    "        return dS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Softmax:\n",
    "    # For Training\n",
    "    def __init__(self):\n",
    "        self.num_examples = 0\n",
    "    \n",
    "    def forward(self, S):\n",
    "        self.num_examples = S.shape[0]\n",
    "        exp_S = np.exp(S)\n",
    "        Z = exp_S / np.sum(exp_S, axis = 1, keepdims = True)\n",
    "        return Z\n",
    "\n",
    "    def backward(self, S, y): # Note: y instead of dZ\n",
    "        probs = Z = self.forward(S)\n",
    "        for i in range(len(y)):\n",
    "            true_label = y[i]\n",
    "            probs[i][true_label] -= 1 # see equation above\n",
    "        dS = probs\n",
    "        return dS\n",
    "    \n",
    "    # For evaluation    \n",
    "    def forward_loss(self, Z, y):\n",
    "        probs = Z\n",
    "        log_probs = []\n",
    "        for prob, true_label in zip(probs, y):\n",
    "            log_probs.append(np.log(prob[true_label]))\n",
    "        avg_cross_entropy_loss = - 1. / self.num_examples * np.sum(log_probs) # see equation above\n",
    "        return avg_cross_entropy_loss\n",
    "    \n",
    "    # For prediction\n",
    "    def predict(self, Z):\n",
    "        return np.argmax(Z, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "for\\ each\\ sample\\ i: \n",
    "$\n",
    "$$\n",
    "\\hat{y_{k}} = softmax(S_1, S_2, ..., S_{k}),\\ k\\ is\\ class\\ index \\\\\n",
    "\\mathbf{E} = - \\sum_{i=1}^N y_{ik} log(\\hat{y_{ik}} )\\\\\n",
    "\\frac{\\partial\\mathbf{E}}{\\partial S_{k}} = {\\hat y}_{k} - y_{k},\\ where\\ y_{k} = 0, 1\\\\\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Normalization\n",
    "- **Idea**: \n",
    "    - Normalize the inputs before activation function / after w*x + b\n",
    "    - Differentiable operation\n",
    "    - Robust to bad initialization\n",
    "    \n",
    "    \n",
    "- **Advantages**: \n",
    "    - Faster training;\n",
    "    - Allow scale and shift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://kratzert.github.io/images/bn_backpass/bn_algorithm.PNG\" width=\"400\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class BatchNorm:\n",
    "    def __init__(self):\n",
    "        self.cache = ()\n",
    "        \n",
    "    def forward(self, X, gamma, beta, eps):\n",
    "        num_examples = X.shape[0]\n",
    "        \n",
    "        mu_B = 1. / num_examples * np.sum(X, axis = 0)\n",
    "        X_mu = X - mu_B\n",
    "        var_B = 1. / num_examples * np.sum(  X_mu ** 2, axis = 0 )\n",
    "        sqrt_var_B = np.sqrt(var_B + eps)\n",
    "        i_sqrt_var_B = 1. / sqrt_var_B\n",
    "        X_hat =  X_mu * i_sqrt_var_B\n",
    "        gammaX = gamma * X_hat\n",
    "        DZ = gammaX + beta\n",
    "        \n",
    "        self.cache = (X_hat, X_mu, gamma, i_sqrt_var_B, sqrt_var_B, var_B, eps)\n",
    "        return DZ\n",
    "    \n",
    "    def backward(self, dDZ):\n",
    "        num_examples = dDZ.shape[0]\n",
    "        X_hat, X_mu, gamma, i_sqrt_var_B, sqrt_var_B, var_B, eps = self.cache\n",
    "        \n",
    "        # scale and shift\n",
    "        dbeta = np.sum(dDZ, axis = 0)\n",
    "        dgammaX = dDZ\n",
    "        dgamma = np.sum(dgammaX * X_hat, axis = 0)\n",
    "        dXhat = dgammaX * gamma\n",
    "        \n",
    "        # Standardize\n",
    "        di_sqrt_var_B = np.sum(dXhat * X_mu, axis = 0)\n",
    "        d_x_mu_2 = dXhat * i_sqrt_var_B\n",
    "        dsqrt_var_B = -1. / (sqrt_var_B ** 2) * di_sqrt_var_B\n",
    "        dvar_B = 0.5 * 1. / np.sqrt(var_B + eps) * dsqrt_var_B\n",
    "\n",
    "        # Batch variance\n",
    "        dsquare = 1. / num_examples * np.ones_like(dDZ) * dvar_B\n",
    "        d_x_mu_1 = 2 * X_mu * dsquare\n",
    "        \n",
    "        # Batch mean\n",
    "        d_x_mu = d_x_mu_2 + d_x_mu_1 # d(f(x)g(x)) = f(x)g'(x) = f'(x)g(x)\n",
    "        dmu = -1. * np.sum(d_x_mu, axis = 0)\n",
    "        dx_2 = d_x_mu\n",
    "        dx_1 = 1. / num_examples * np.ones_like(dDZ) * dmu\n",
    "        dx = dx_2 + dx_1\n",
    "        \n",
    "        return dx, dgamma, dbeta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "# Reference: https://github.com/wiseodd/hipsternet/blob/master/hipsternet\n",
    "class Convolution:\n",
    "    def __init__(self):\n",
    "        self.cache = ()\n",
    "        \n",
    "    def forward(self, X, W, b, stride=1, padding=1):\n",
    "        \n",
    "        # W: (num_filters, num_channels, filter_h, filter_w)\n",
    "        # X: (num_examples, num_channels, height, width)\n",
    "        n_filters, d_filter, h_filter, w_filter = W.shape\n",
    "        n_x, d_x, h_x, w_x = X.shape\n",
    "        \n",
    "        # Calculate Output Shape\n",
    "        h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
    "        w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
    "        \n",
    "        if not h_out.is_integer() or not w_out.is_integer():\n",
    "            raise Exception('Invalid output dimension!')\n",
    "\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "        \n",
    "        # Column-ize W and X\n",
    "        # W_col: ( num_filters, num_channels * filter_h * filter_w) \n",
    "        # X_col: ( num_channels * filter_h * filter_w, h_out * w_out * num_examples)\n",
    "        \n",
    "        W_col = W.reshape(n_filters, -1)\n",
    "        X_col = im2col_indices(X, h_filter, w_filter, padding=padding, stride=stride)\n",
    "        \n",
    "        # Matrix Multiply\n",
    "        # W_col * X_col: (num_filters, h_out * w_out * num_examples)\n",
    "        # b: (num_filters, 1)\n",
    "        # WX + b: (num_filters, h_out * w_out * num_examples)\n",
    "        \n",
    "        out = np.matmul(W_col, X_col) + b\n",
    "        out = out.reshape(n_filters, h_out, w_out, n_x)\n",
    "        out = out.transpose(3, 0, 1, 2)\n",
    "\n",
    "        # out: (num_examples, num_filters, h_out, w_out)\n",
    "        self.cache = (X, W, b, stride, padding, X_col)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "    def backward(self, dout):\n",
    "        X, W, b, stride, padding, X_col = self.cache\n",
    "        n_filter, d_filter, h_filter, w_filter = W.shape\n",
    "\n",
    "        db = np.sum(dout, axis=(0, 2, 3))\n",
    "        db = db.reshape(n_filter, -1)\n",
    "\n",
    "        dout_reshaped = dout.transpose(1, 2, 3, 0).reshape(n_filter, -1)\n",
    "        dW = np.matmul(dout_reshaped,X_col.T)\n",
    "        dW = dW.reshape(W.shape)\n",
    "\n",
    "        W_reshape = W.reshape(n_filter, -1)\n",
    "        dX_col = np.matmul(W_reshape.T,dout_reshaped)\n",
    "        dX = col2im_indices(dX_col, X.shape, h_filter, w_filter, padding=padding, stride=stride)\n",
    "\n",
    "        return dX, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Maxpool:\n",
    "    def forward(self, X_col):\n",
    "        # X_reshaped: (num_examples * num_filters, 1, new_h_out, new_w_out)\n",
    "        # for example: X.shape = (10,5,4,4), size = stride = 2, (new) h_out = w_out = 2\n",
    "        # for example: X_reshaped = (50, 1, 2, 2)\n",
    "        # X_col: (1 * size * size, num_filters(new_num_channels) * h_out * w_out * num_examples)\n",
    "        # for example, X_col = (1*2*2, 5*2*2*10) = (4, 200)\n",
    "        # max_idx.shape = out.shape = (200, )\n",
    "        max_idx = np.argmax(X_col, axis=0) # for every (size * size) cells, select one max [0,1,2,3,1,2,...]\n",
    "        out = X_col[max_idx, range(X_col.shape[1])]\n",
    "        return out, max_idx\n",
    "\n",
    "    def backward(self, dX_col, dout_col, max_idx):\n",
    "        # Only max value got local gradient = 1\n",
    "        dX_col[max_idx, range(dout_col.size)] = 1.0 * dout_col\n",
    "        return dX_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Propagation.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Propagation.py\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_fun = Maxpool()):\n",
    "        self.cache = ()\n",
    "        self.max_idx = ()\n",
    "        self.pool_fun = pool_fun\n",
    "        \n",
    "    def forward(self, X, size = 2, stride = 2):\n",
    "        # Calculate new shape after pooling\n",
    "        \n",
    "        # X_shape = (num_examples, num_filters, h_out, w_out) from CNN layer\n",
    "        # for example: X = (10,5,4,4), size=stride=2, h_out=w_out=2\n",
    "        n, d, h, w = X.shape\n",
    "        h_out = (h - size) / stride + 1\n",
    "        w_out = (w - size) / stride + 1\n",
    "\n",
    "        if not w_out.is_integer() or not h_out.is_integer():\n",
    "            raise Exception('Invalid output dimension!')\n",
    "\n",
    "        h_out, w_out = int(h_out), int(w_out)\n",
    "\n",
    "        # X_reshaped: (num_examples * num_filters, new_h_out, new_w_out)\n",
    "        X_reshaped = X.reshape(n * d, 1, h, w)\n",
    "        \n",
    "        # X_col: (num_filters(new_num_channels) * size * size, h_out, w_out * num_examples)\n",
    "        X_col = im2col_indices(X_reshaped, size, size, padding=0, stride=stride)\n",
    "\n",
    "        out, self.max_idx = self.pool_fun.forward(X_col)\n",
    " \n",
    "        out = out.reshape(h_out, w_out, n, d)\n",
    "        out = out.transpose(2, 3, 0, 1)\n",
    "        # out: (num_examples, num_filters (new_num_channels), new_h_out, new_w_out)\n",
    "        \n",
    "        self.cache = (X, size, stride, X_col)\n",
    "\n",
    "        return out  \n",
    "    \n",
    "    \n",
    "    def backward(self, dout):\n",
    "        X, size, stride, X_col = self.cache\n",
    "        n, d, w, h = X.shape\n",
    "\n",
    "        dX_col = np.zeros_like(X_col)\n",
    "        dout_col = dout.transpose(2, 3, 0, 1).ravel()\n",
    "\n",
    "        dX = self.pool_fun.backward(dX_col, dout_col, self.max_idx)\n",
    "        dX = col2im_indices(dX_col, (n * d, 1, h, w), size, size, padding=0, stride=stride)\n",
    "        dX = dX.reshape(X.shape)\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Optimization.py\n",
    "import numpy as np\n",
    "# Treat all elements of dX as a whole\n",
    "#  Intuition: \n",
    "#  If gradient direction not changed, increase update, faster convergence\n",
    "#  If gradient direction changed, reduce update, reduce oscillation\n",
    "\n",
    "def VanillaUpdate(x, dx, learning_rate):\n",
    "    x += -learning_rate * dx\n",
    "    return x\n",
    "        \n",
    "# Vanilla version\n",
    "#self.W += -learning_rate * self.dW + (- lambda_ * self.W)\n",
    "#self.b += -learning_rate * self.db\n",
    "#self.gamma += -learning_rate * self.dgamma\n",
    "#self.beta  += -learning_rate * self.dbeta\n",
    "\n",
    "def MomentumUpdate(x, dx, v, learning_rate, mu):\n",
    "    v = mu * v - learning_rate * dx # integrate velocity, mu's typical value is about 0.9\n",
    "    x += v # integrate position     \n",
    "    return x, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Optimization.py\n",
    "\n",
    "# Treat each element of dX adaptively\n",
    "# Intuition:\n",
    "# 1. Those dx receiving infrequent updates should have higher learning rate. vice versa \n",
    "# 2. We don't want: the gradients accumulate, and the learning rate monotically decrease, \n",
    "# 2. We want: modulates the learning rate of each weight based on the magnitudes of its gradient\n",
    "# 3. Still want to use \"momentum-like\" update to get a smooth gradient\n",
    "\n",
    "# 1. AdaGrad\n",
    "def AdaGrad(x, dx, learning_rate, cache, eps):\n",
    "    cache += dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps) # (usually set somewhere in range from 1e-4 to 1e-8)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2. RMSprop\n",
    "def RMSprop(x, dx, learning_rate, cache, eps, decay_rate): #Here, decay_rate typical values are [0.9, 0.99, 0.999]\n",
    "    cache = decay_rate * cache + (1 - decay_rate) * dx**2\n",
    "    x += - learning_rate * dx / (np.sqrt(cache) + eps)\n",
    "    return x, cache\n",
    "    \n",
    "# 1+2+3. Adam\n",
    "def Adam(x, dx, learning_rate, m, v, t, beta1, beta2, eps):\n",
    "    m = beta1*m + (1-beta1)*dx # Smooth gradient\n",
    "    #mt = m / (1-beta1**t) # bias-correction step\n",
    "    v = beta2*v + (1-beta2)*(dx**2) # keep track of past updates\n",
    "    #vt = v / (1-beta2**t) # bias-correction step\n",
    "    x += - learning_rate * m / (np.sqrt(v) + eps) # eps = 1e-8, beta1 = 0.9, beta2 = 0.999   \n",
    "    return x, m, v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Optimization.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Optimization.py\n",
    "\n",
    "class WeightUpdate:\n",
    "    def __init__(self, init_value):\n",
    "        self.val = init_value\n",
    "        self.cache = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.m = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.v = np.zeros_like(self.val, dtype=np.float64)\n",
    "        self.t = 0\n",
    "    \n",
    "    def Update(self, d, learning_rate, lambda_ , method):\n",
    "        \n",
    "        old_val = self.val\n",
    "        if method == 'Vanilla':\n",
    "            self.val = VanillaUpdate(self.val, d, learning_rate)\n",
    "        elif method == 'MomentumUpdate':\n",
    "            self.val, self.v = MomentumUpdate(self.val, d, self.v, learning_rate, mu = 0.9)\n",
    "        elif method == 'AdaGrad':\n",
    "            self.val, self.cache = AdaGrad(self.val, d, learning_rate, self.cache, eps = 1e-5)\n",
    "        elif method == 'RMSprop':\n",
    "            self.val, self.cache = AdaGrad(self.val, d, learning_rate, self.cache, eps = 1e-5, decay_rate = 0.99)\n",
    "        elif method == 'Adam':\n",
    "            self.val, self.m, self.v = Adam(self.val, d, learning_rate, self.m, self.v, self.t, beta1 = 0.9, beta2 = 0.999, eps = 1e-8)  \n",
    "            self.t += 1\n",
    "            \n",
    "        # Regularization\n",
    "        self.val -= lambda_ * old_val\n",
    "        return self.val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Layer.py\n",
    "from script.Propagation import *\n",
    "class BaseLayer:\n",
    "    def __init__(self):\n",
    "        self.isfirst = False\n",
    "        self.islast = False\n",
    "        self.before = None\n",
    "        self.after = None\n",
    "        self.gamma, self.beta = 1,0\n",
    "        self.dgamma, self.dbeta = 0,0\n",
    "\n",
    "    def set_first_layer(self, input):\n",
    "        self.isfirst = True\n",
    "        self.X = input\n",
    "        \n",
    "    def set_last_layer(self, y):\n",
    "        self.islast = True\n",
    "        self.y = y\n",
    "    \n",
    "    def initialize_Wb(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward_propagation(self):\n",
    "        raise NotImplementedError()\n",
    "            \n",
    "    def backward_propagation(self):\n",
    "        raise NotImplementedError()\n",
    "    \n",
    "    def update_weight(self, learning_rate, lambda_ , method):\n",
    "        \n",
    "        # Create variable list\n",
    "        self.weights = [self.W, self.b, self.gamma, self.beta]\n",
    "        self.ds = [self.dW, self.db, self.dgamma, self.dbeta]\n",
    "\n",
    "        # First Time\n",
    "        if not hasattr(self, 'updates'):      \n",
    "            self.updates = []\n",
    "            for weight in self.weights:\n",
    "                self.updates.append(WeightUpdate(weight))\n",
    "        \n",
    "        # Calculate update for each iteration\n",
    "        new_weights = []\n",
    "        for weight_update, d in zip(self.updates, self.ds):\n",
    "            new_weights.append(weight_update.Update(d, learning_rate, lambda_, method))\n",
    "        \n",
    "        # Update weights\n",
    "        self.W, self.b, self.gamma, self.beta = new_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully-Connected Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layer.py\n",
    "\n",
    "class FC(BaseLayer):\n",
    "    def __init__(self, activation_function, num_neurons, batch_norm = False, dropout_p = 1.0): # p = 1 means no dropout\n",
    "        super().__init__()\n",
    "        self.dim = num_neurons\n",
    "        self.activation = activation_function\n",
    "        self.batch_norm = batch_norm\n",
    "        if batch_norm:\n",
    "            self.batchnorm = BatchNorm()\n",
    "        self.p = dropout_p\n",
    "        self.name = 'FC'\n",
    "        \n",
    "    def initialize_Wb(self):\n",
    "        before_dim = self.X.shape[1]\n",
    "        self.W = np.random.randn(before_dim, self.dim) / np.sqrt(before_dim) # see notes above\n",
    "        self.b = np.random.randn(self.dim).reshape(1, self.dim) # see notes above\n",
    "        \n",
    "    def forward_propagation(self):\n",
    "        if not self.isfirst:\n",
    "            self.X = self.before.Z\n",
    "        \n",
    "        if not hasattr(self, 'W'):\n",
    "            self.initialize_Wb() \n",
    "            \n",
    "        self.mask = np.random.rand(*self.W.shape) < self.p / self.p\n",
    "        self.W *= self.mask\n",
    "        self.WX = Mul().forward( self.W, self.X )\n",
    "        self.S = Add().forward( self.WX, self.b)\n",
    "        if self.batch_norm:\n",
    "            self.SZ = self.batchnorm.forward( self.S, self.gamma, self.beta, eps = 0)\n",
    "        else:\n",
    "            self.SZ = self.S\n",
    "        self.Z = self.activation.forward(self.SZ)\n",
    "            \n",
    "    def backward_propagation(self):\n",
    "        if self.islast:\n",
    "            self.dZ = self.y\n",
    "        \n",
    "        self.dSZ = self.activation.backward(self.SZ, self.dZ)\n",
    "        if self.batch_norm:\n",
    "            self.dS, self.dgamma, self.dbeta = self.batchnorm.backward(self.dSZ)\n",
    "        else:\n",
    "            self.dS = self.dSZ\n",
    "            self.dgamma, self.dbeta = 0,0\n",
    "        self.db, self.dWX = Add().backward(self.WX, self.b, self.dS)\n",
    "        self.dW, self.dX = Mul().backward(self.W, self.X, self.dWX)\n",
    "        self.dW *= self.mask\n",
    "        \n",
    "        if not self.isfirst:\n",
    "            self.before.dZ = self.dX\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layer.py\n",
    "\n",
    "class Softmax_Classifier(FC):\n",
    "    def __init__(self, activation_function, num_neurons, batch_norm = False, dropout_p = 1.0):\n",
    "        super().__init__(activation_function, num_neurons, batch_norm, dropout_p)\n",
    "        self.name = 'Softmax'\n",
    "        \n",
    "    def calculate_loss(self):\n",
    "        loss = self.activation.forward_loss(self.Z, self.y)\n",
    "        return loss\n",
    "            \n",
    "    def predict(self):\n",
    "        return self.activation.predict(self.Z)\n",
    "    \n",
    "    def calculate_acc(self): \n",
    "        pred = self.predict()\n",
    "        return sum( pred == self.y ) / len(self.y)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to ./script/Layer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile -a ./script/Layer.py\n",
    "  \n",
    "\n",
    "class CNN(BaseLayer):\n",
    "    def __init__(self, activation_function, params):\n",
    "        super().__init__()\n",
    "        self.activation = activation_function\n",
    "        self.conv = Convolution()\n",
    "        self.pooling = Pooling()\n",
    "        self.name = 'CNN'\n",
    "       \n",
    "        self.num_filters,  self.filter_h, self.filter_w = params['num_filters'], params['filter_h'],params['filter_w']\n",
    "        self.filter_stride, self.filter_padding = params['filter_stride'],params['filter_padding']\n",
    "        self.pooling_size, self.pooling_stride = params['pooling_size'],params['pooling_stride']\n",
    "    \n",
    "    def initialize_Wb(self):\n",
    "        # W: (num_filters, num_channels, filter_h, filter_w)\n",
    "        # X: (num_examples, num_channels, height, width)\n",
    "        # b: (num_filters, 1)\n",
    "        if self.isfirst:\n",
    "            self.num_channels = self.X.shape[-3] \n",
    "        else:\n",
    "            self.num_channels = self.before.num_filters\n",
    "        self.W = np.random.randn(self.num_filters, self.num_channels, self.filter_h, self.filter_w) / np.sqrt(self.num_filters / 2.)\n",
    "        self.b = np.random.randn(self.num_filters).reshape(self.num_filters,1)\n",
    "        \n",
    "    def forward_propagation(self):\n",
    "        if not self.isfirst:\n",
    "            self.X = self.before.Z\n",
    "\n",
    "        if not hasattr(self, 'W'):\n",
    "            self.initialize_Wb() \n",
    "            \n",
    "        self.D = self.conv.forward(self.X, self.W, self.b, \n",
    "                                   stride = self.filter_stride, \n",
    "                                   padding = self.filter_padding)\n",
    "        self.Z_conv = self.activation.forward(self.D)\n",
    "        self.Z_conv_pool = self.pooling.forward(self.Z_conv, \n",
    "                                                size = self.pooling_size, \n",
    "                                                stride = self.pooling_stride)\n",
    "        \n",
    "        if self.after.name != 'CNN':\n",
    "            self.Z_pool_flat = self.Z_conv_pool.ravel().reshape(self.X.shape[0], -1)\n",
    "            self.Z = self.Z_pool_flat\n",
    "        else:\n",
    "            self.Z = self.Z_conv_pool\n",
    "    \n",
    "    def backward_propagation(self):\n",
    "        if self.after.name != 'CNN':\n",
    "            self.dZ_conv_pool = self.dZ.ravel().reshape(self.Z_conv_pool.shape)\n",
    "        else:\n",
    "            self.dZ_conv_pool = self.dZ\n",
    "            \n",
    "        self.dZ_conv = self.pooling.backward(self.dZ_conv_pool)\n",
    "        self.dX, self.dW, self.db  = self.conv.backward(self.dZ_conv)\n",
    "        \n",
    "        if not self.isfirst:\n",
    "            self.before.dZ = self.dX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/Network.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./script/Network.py\n",
    "import numpy as np\n",
    "from script.Layer import *\n",
    "class Network:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "        self.input = []\n",
    "        self.y = []\n",
    "        \n",
    "    def add(self, new_layer):\n",
    "        if self.layers:\n",
    "            self.layers[-1].after = new_layer\n",
    "            new_layer.before = self.layers[-1]\n",
    "        self.layers.append(new_layer)\n",
    "    \n",
    "    def load_data(self, input, y):\n",
    "        self.layers[0].set_first_layer(input)\n",
    "        self.layers[-1].set_last_layer(y)\n",
    "        \n",
    "    def initialize(self, input, y, batch_size):\n",
    "        self.input = input\n",
    "        self.y = y\n",
    "        self.load_data(input[:batch_size,:], y[:batch_size])\n",
    "        #for layer in self.layers:\n",
    "        #    layer.initialize_Wb()\n",
    "\n",
    "    def train(self, num_iter, learning_rate, batch_size, rand_, lambda_, optimizer = 'Vanilla', Val_X = None, Val_y = None, \n",
    "             CAL_STEP = 100, PRINT_STEP = 100):\n",
    "        \n",
    "        train_loss = []\n",
    "        train_acc = []\n",
    "        val_loss = []\n",
    "        val_acc = []\n",
    "        \n",
    "        for i in range(num_iter):    \n",
    "            # Calculate batch index\n",
    "            if not rand_:\n",
    "                idx = list(range(self.input.shape[0]))\n",
    "            else:\n",
    "                idx = np.random.randint(self.input.shape[0], size = batch_size)\n",
    "            \n",
    "            self.load_data(self.input[idx,:], self.y[idx])\n",
    "            \n",
    "            # Forward Propagation\n",
    "            for layer in self.layers:\n",
    "                layer.forward_propagation()\n",
    "                \n",
    "            # Print Traing Acc/Loss\n",
    "            if (i % CAL_STEP == 0):\n",
    "                t_loss = self.layers[-1].calculate_loss()\n",
    "                t_acc  = self.layers[-1].calculate_acc()\n",
    "                train_loss.append(t_loss)\n",
    "                train_acc.append(t_acc)\n",
    "                \n",
    "            if (i % PRINT_STEP == 0):\n",
    "                print('Train at Iter {0:2d}: loss - {1:.3f}, Acc - {2:.3f}'.format(i, t_loss, t_acc))\n",
    "                \n",
    "            # Backward Propagation\n",
    "            for layer in self.layers[::-1]:\n",
    "                layer.backward_propagation()\n",
    "                layer.update_weight(learning_rate, lambda_ = lambda_ , method = optimizer)\n",
    "            \n",
    "            # Print Validation Acc/Loss\n",
    "            if (i % CAL_STEP == 0 and Val_X is not None):\n",
    "                v_acc, v_loss = self.evaluate(Val_X, Val_y)\n",
    "                val_loss.append(v_loss)\n",
    "                val_acc.append(v_acc) \n",
    "                \n",
    "            if (i % PRINT_STEP == 0 and Val_X is not None):\n",
    "                print('Validation at Iter {0:2d}: loss - {1:.3f}, Acc - {2:.3f}'.format(i, v_loss, v_acc))\n",
    "\n",
    "        # Finally return loss list\n",
    "        return train_loss, train_acc, val_loss, val_acc\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.load_data(X, y = None)\n",
    "        for layer in self.layers:\n",
    "            layer.forward_propagation()\n",
    "        return layers[-1].predict()\n",
    "        \n",
    "    def evaluate(self, X, y):\n",
    "        self.load_data(X, y)\n",
    "        for layer in self.layers:\n",
    "            layer.forward_propagation()\n",
    "        loss = self.layers[-1].calculate_loss()\n",
    "        acc  = self.layers[-1].calculate_acc()\n",
    "        return acc, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7, 4, 4) (10, 5, 4, 4) (10, 5, 4, 4) (10, 5, 2, 2) (10, 20)\n"
     ]
    }
   ],
   "source": [
    "from script.Propagation import *\n",
    "from script.Layer import *\n",
    "\n",
    "# Test Case\n",
    "# Default padding: SAME\n",
    "NUM_EXAMPLES = 10\n",
    "NUM_CHANNELS = 7\n",
    "NUM_FILTERS = 5\n",
    "INPUT_HEIGHT = 4\n",
    "INPUT_WIDTH =  4\n",
    "FILTER_HEIGHT = 1\n",
    "FILTER_WIDTH = 1\n",
    "STRIDE = 1\n",
    "PAD = 0\n",
    "\n",
    "n_x, d_x, h_x, w_x  = x_shape = (NUM_EXAMPLES, NUM_CHANNELS, INPUT_HEIGHT, INPUT_WIDTH) # num_examples, depth(channels), height, width\n",
    "n_filters, d_filter, h_filter, w_filter = w_shape = (NUM_FILTERS, NUM_CHANNELS, FILTER_HEIGHT, FILTER_WIDTH) # num_filters, depth(channels), kernel_height, kernel_width\n",
    "b_shape = (NUM_FILTERS, 1)\n",
    "x = np.linspace(-0.1, 0.5, num=np.prod(x_shape)).reshape(x_shape)\n",
    "w = np.linspace(-0.2, 0.3, num=np.prod(w_shape)).reshape(w_shape)\n",
    "b = np.linspace(-0.1, 0.2, num=np.prod(b_shape)).reshape(b_shape)\n",
    "\n",
    "# Test 1:\n",
    "\n",
    "params = {'num_filters': NUM_FILTERS,\n",
    "          'filter_h': FILTER_HEIGHT,\n",
    "          'filter_w': FILTER_WIDTH,\n",
    "          'filter_stride': STRIDE,\n",
    "          'filter_padding': PAD,\n",
    "          'pooling_size': 2,\n",
    "          'pooling_stride': 2\n",
    "         }\n",
    "\n",
    "cnn = CNN(activation_function= ReLU(), params = params )\n",
    "\n",
    "# Test 2:\n",
    "conv = Convolution()\n",
    "relu = ReLU()\n",
    "pool = Pooling()\n",
    "D = conv.forward(x, w, b, stride = STRIDE, padding = PAD)\n",
    "Z_conv = relu.forward(D)\n",
    "Z_conv_pool = pool.forward(Z_conv, size = 2, stride = 2)\n",
    "Z_pool_flat = Z_conv_pool.ravel().reshape(x.shape[0], -1)\n",
    "print(x.shape, D.shape, Z_conv.shape, Z_conv_pool.shape, Z_pool_flat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "12px",
    "width": "160px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
